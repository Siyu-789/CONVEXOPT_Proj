while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
alpha <- 0.01
beta <- 0.5
max_k <- 10
eta <- 0.001
n <- 100
# 牛顿方法的初始化
x <- rep(0, n)
A <- matrix(runif(n * n), nrow = n)
values <- numeric(max_k)
for (iter in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[i] <- val
# Df(x)
grad <- A %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)
# Hessian matrix
diag_Ax <- diag(1 / (1 - Ax)^2)
diag_x_plus <- diag(1 / (1 + x)^2)
diag_x_minus <- diag(1 / (1 - x)^2)
hessian <- A %*% diag_Ax %*% t(A) + diag_x_plus + diag_x_minus
# Newton step
v <- solve(hessian, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < ntol) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
alpha <- 0.01
beta <- 0.5
max_k <- 10
eta <- 0.001
n <- 100
# 牛顿方法的初始化
x <- rep(0, n)
A <- matrix(runif(n * n), nrow = n)
values <- numeric(max_k)
for (iter in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[i] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(d^2) %*% A + diag(1 / (1 + x)^2) + diag(1 / (1 - x)^2)
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.1
m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (iter in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(d^2) %*% A + diag(1 / (1 + x)^2) + diag(1 / (1 - x)^2)
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
m <- 10
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.1
m <- 10
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (iter in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(d^2) %*% A + diag(1 / (1 + x)^2) + diag(1 / (1 - x)^2)
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.1
m <- 10
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (k in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(as.vector(d^2)) %*% A + diag(as.vector(1 / (1 + x)^2)) + diag(as.vector(1 / (1 - x)^2))
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
# 绘制目标函数值随迭代次数变化的图
plot(values, type = 'b', col = 'blue', xlab = 'Iteration', ylab = 'Objective function value', main = 'Objective Function During Newton Method')
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.01
m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (k in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(as.vector(d^2)) %*% A + diag(as.vector(1 / (1 + x)^2)) + diag(as.vector(1 / (1 - x)^2))
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
# 绘制目标函数值随迭代次数变化的图
plot(values, type = 'b', col = 'blue', xlab = 'Iteration', ylab = 'Objective function value', main = 'Objective Function During Newton Method')
alpha <- 0.01
beta <- 0.5
max_k <- 10
eta <- 0.0001
m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (k in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(as.vector(d^2)) %*% A + diag(as.vector(1 / (1 + x)^2)) + diag(as.vector(1 / (1 - x)^2))
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
# 绘制目标函数值随迭代次数变化的图
plot(values, type = 'b', col = 'blue', xlab = 'Iteration', ylab = 'Objective function value', main = 'Objective Function During Newton Method')
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.0001
m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (k in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(as.vector(d^2)) %*% A + diag(as.vector(1 / (1 + x)^2)) + diag(as.vector(1 / (1 - x)^2))
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- t(grad) %*% v
# Check for stopping criterion
if (abs(fprime) < eta) {
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", iter, "Value:", val))
}
# 绘制目标函数值随迭代次数变化的图
plot(values, type = 'b', col = 'blue', xlab = 'Iteration', ylab = 'Objective function value', main = 'Objective Function During Newton Method')
plot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^k)', main = 'Objective Function During Newton Method')
plot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^k)')
plot(1:k, values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^k)')
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.0001
m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)
values <- numeric(max_k)
for (k in 1:max_k) {
# f(x)
Ax <- A %*% x
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
values[k] <- val
# Df(x)
d <- 1 / (1 - Ax)
grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
# Hessian matrix
H <- t(A) %*% diag(as.vector(d^2)) %*% A + diag(as.vector(1 / (1 + x)^2)) + diag(as.vector(1 / (1 - x)^2))
# Newton step
v <- solve(H, grad)
# Newton decrement
fprime <- sum(grad * v)
# Check for stopping criterion
if (abs(fprime) < eta) {
values <- values[1:k]  # Adjust the length of values vector
break
}
# Line search
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# Update x
x <- new_x
# Optionally print progress
print(paste("Iteration:", k, "Value:", val))  # Use `k` instead of `iter`
}
# Plot the objective function value by iteration
if (k == 1) { values <- values[1] }  # If only one iteration is performed
plot(1:k, values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^k)')
norm(grad_norm)
gamma <- 0.01
beta <- 0.5
max_i <- 10
eta <- 0.001
# size
m <- 200
n <- 100
# initialization
x <- rep(1, n)
A <- matrix(runif(m * n), nrow = m)
Ax <- A %*% x
# storage
values <- numeric(max_i)
t <- numeric(max_i)
n_i <- numeric(max_i)
# for loop
for (i in 1:max_i) {
# f(x)
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
# Df(x)
grad <- t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)
# store iteration values
n_i[i] <- i
values[i] <- val
norm(grad_norm)
gamma <- 0.01
beta <- 0.5
max_i <- 10
eta <- 0.001
# size
m <- 200
n <- 100
# initialization
x <- rep(1, n)
A <- matrix(runif(m * n), nrow = m)
Ax <- A %*% x
# storage
values <- numeric(max_i)
steplen <- numeric(max_i)
n_i <- numeric(max_i)
# for loop
for (i in 1:max_i) {
# f(x)
val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
# Df(x)
grad <- t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)
# store iteration values
n_i[i] <- i
values[i] <- val
# stop statement: ||Df(x)||_2 \leq \eta
if (norm(grad_norm) < eta) {
values <- values[1:i]
n_i <- n_i[1:i]
break
}
# direction of gradient method
v <- -grad
# backtracking line search: loop for t
t <- 1
while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
t <- beta * t
}
# Ensure to update Ax only once per iteration
new_x <- x + t * v
new_Ax <- A %*% new_x
while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * sum(grad * v)) {
t <- beta * t
new_x <- x + t * v
new_Ax <- A %*% new_x
}
# iteration
x <- new_x
Ax <- new_Ax
# store iteration step length
steplen[i] <- t
# Optionally print progress
cat("k:", k, "Value:", val, "\n")
}
plot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'Objective function value', main = 'Objective Function During Gradient Descent')
plot(grad_norms, type = 'b', col = 'red', xlab = 'k', ylab = 'Gradient norm', main = 'Gradient Norm During Gradient Descent')
