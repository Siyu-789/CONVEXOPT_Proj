# Gradient Method & Newton Method

## Problem 9.30 (10 points)

### a

values

```{r}
gamma <- 0.01
beta <- 0.5
max_i <- 10
eta <- 0.001

# size
m <- 200
n <- 100
# initialization
x <- rep(1, n)
A <- matrix(runif(m * n), nrow = m)

Ax <- A %*% x

# storage
values <- numeric(max_i)
steplen <- numeric(max_i)
n_i <- numeric(max_i)

# for loop
for (i in 1:max_i) {
  # f(x)
  val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
  
  # Df(x)
  grad <- t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)
  
  # store iteration values
  n_i[i] <- i
  values[i] <- val
  
  # stop statement: ||Df(x)||_2 \leq \eta
  if (norm(grad_norm) < eta) {
    values <- values[1:i]  
    n_i <- n_i[1:i]
    break
  }
  
  # direction of gradient method
  v <- -grad
  
  # backtracking line search: loop for t
  t <- 1
  while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
    t <- beta * t
  }
  
  # Ensure to update Ax only once per iteration
  new_x <- x + t * v
  new_Ax <- A %*% new_x
  
  while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + gamma * t * sum(grad * v)) {
    t <- beta * t
    new_x <- x + t * v
    new_Ax <- A %*% new_x
  }
  
  # iteration
  x <- new_x
  Ax <- new_Ax
  
  # store iteration step length
  steplen[i] <- t
  # Optionally print progress
  cat("k:", k, "Value:", val, "\n")
}


plot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'Objective function value', main = 'Objective Function During Gradient Descent')


plot(grad_norms, type = 'b', col = 'red', xlab = 'k', ylab = 'Gradient norm', main = 'Gradient Norm During Gradient Descent')


```

```{r}
# different size
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.1

m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)

Ax <- A %*% x

# storage
values <- numeric(max_k)
grad_norms <- numeric(max_k) 

# for loop
for (k in 1:max_k) {
  # f(x)
  val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
  
  # Df(x)
  grad <- t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)
  
  # store iteration values
  grad_norm <- norm(grad, type="2")
  values[k] <- val
  grad_norms[k] <- grad_norm
  
  # stop statement: ||Df(x)||_2 \leq \eta
  if (grad_norm < eta) {
    values <- values[1:k]  
    grad_norms <- grad_norms[1:k] 
    break
  }
  
  # direction of gradient method
  v <- -grad
  
  # backtracking line search: loop for t
  t <- 1
  while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
    t <- beta * t
  }
  
  # Ensure to update Ax only once per iteration
  new_x <- x + t * v
  new_Ax <- A %*% new_x
  
  while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * sum(grad * v)) {
    t <- beta * t
    new_x <- x + t * v
    new_Ax <- A %*% new_x
  }
  
  # iteration
  x <- new_x
  Ax <- new_Ax
  
  # Optionally print progress
  cat("k:", k, "Value:", val, "\n")
}


plot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'Objective function value', main = 'Objective Function During Gradient Descent')


plot(grad_norms, type = 'b', col = 'red', xlab = 'k', ylab = 'Gradient norm', main = 'Gradient Norm During Gradient Descent')
```

### b

```{r}
alpha <- 0.01
beta <- 0.5
max_k <- 5
eta <- 0.0001

m <- 20
n <- 10
# initialization
x <- rep(0, n)
A <- matrix(runif(m * n), nrow = m)

values <- numeric(max_k)

for (k in 1:max_k) {
  # f(x)
  Ax <- A %*% x
  val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
  
  values[k] <- val
   
  # Df(x)
  d <- 1 / (1 - Ax)
  grad <- t(A) %*% d - 1 / (1 + x) + 1 / (1 - x)
  
  # Hessian matrix
  H <- t(A) %*% diag(as.vector(d^2)) %*% A + diag(as.vector(1 / (1 + x)^2)) + diag(as.vector(1 / (1 - x)^2))
  
  # Newton step
  v <- solve(H, grad)
  
  # Newton decrement
  fprime <- sum(grad * v)
  
  # Check for stopping criterion
  if (abs(fprime) < eta) {
    values <- values[1:k]  # Adjust the length of values vector
    break
  }
  
  # Line search
  t <- 1
  while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
    t <- beta * t
  }
  
  new_x <- x + t * v
  new_Ax <- A %*% new_x
  
  while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * fprime) {
    t <- beta * t
    new_x <- x + t * v
    new_Ax <- A %*% new_x
  }
  
  # Update x
  x <- new_x
  
  # Optionally print progress
  print(paste("Iteration:", k, "Value:", val))  # Use `k` instead of `iter`
}

# Plot the objective function value by iteration
if (k == 1) { values <- values[1] }  # If only one iteration is performed
plot(1:k, values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^k)')
```
