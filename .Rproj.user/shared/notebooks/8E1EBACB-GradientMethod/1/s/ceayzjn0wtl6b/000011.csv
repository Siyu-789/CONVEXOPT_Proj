"0","gamma <- 0.01"
"0","beta <- 0.5"
"0","max_i <- 10"
"0","eta <- 0.001"
"0",""
"0","# size"
"0","m <- 200"
"0","n <- 100"
"0","# initialization"
"0","x <- rep(1, n)"
"0","A <- matrix(runif(m * n), nrow = m)"
"0",""
"0","Ax <- A %*% x"
"0",""
"0","# storage"
"0","values <- numeric(max_i)"
"0","steplen <- numeric(max_i)"
"0","n_i <- numeric(max_i)"
"0",""
"0","# for loop"
"0","for (i in 1:max_i) {"
"0","  # f(x)"
"0","  val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))"
"0","  "
"0","  # Df(x)"
"0","  grad <- t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)"
"0","  "
"0","  # store iteration values"
"0","  n_i[i] <- i"
"0","  values[i] <- val"
"0","  "
"0","  # stop statement: ||Df(x)||_2 \leq \eta"
"0","  if (norm(grad_norm) < eta) {"
"0","    values <- values[1:i]  "
"0","    n_i <- n_i[1:i]"
"0","    break"
"0","  }"
"0","  "
"0","  # direction of gradient method"
"0","  v <- -grad"
"0","  "
"0","  # backtracking line search: loop for t"
"0","  t <- 1"
"0","  while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {"
"0","    t <- beta * t"
"0","  }"
"0","  "
"0","  # Ensure to update Ax only once per iteration"
"0","  new_x <- x + t * v"
"0","  new_Ax <- A %*% new_x"
"0","  "
"0","  while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) - sum(log(1 - new_x)) > val + alpha * t * sum(grad * v)) {"
"0","    t <- beta * t"
"0","    new_x <- x + t * v"
"0","    new_Ax <- A %*% new_x"
"0","  }"
"0","  "
"0","  # iteration"
"0","  x <- new_x"
"0","  Ax <- new_Ax"
"0","  "
"0","  # store iteration step length"
"0","  steplen[i] <- t"
"0","  # Optionally print progress"
"0","  cat(""k:"", k, ""Value:"", val, ""\n"")"
"0","}"
"0",""
"0",""
"0","plot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'Objective function value', main = 'Objective Function During Gradient Descent')"
"0",""
"0",""
"0","plot(grad_norms, type = 'b', col = 'red', xlab = 'k', ylab = 'Gradient norm', main = 'Gradient Norm During Gradient Descent')"
"0",""
"0",""
"2","错误: Incomplete expression: gamma <- 0.01
beta <- 0.5
max_i <- 10
eta <- 0.001

# size
m <- 200
n <- 100
# initialization
x <- rep(1, n)
A <- matrix(runif(m * n), nrow = m)

Ax <- A %*% x

# storage
values <- numeric(max_i)
steplen <- numeric(max_i)
n_i <- numeric(max_i)

# for loop
for (i in 1:max_i) {
  # f(x)
  val <- -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))
  
  # Df(x)
  grad <- t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)
  
  # store iteration values
  n_i[i] <- i
  values[i] <- val
  
  # stop statement: ||Df(x)||_2 \leq \eta
  if (norm(grad_norm) < eta) {
    values <- values[1:i]  
    n_i <- n_i[1:i]
    break
  }
  
  # direction of gradient method
  v <- -grad
  
  # backtracking line search: loop for t
  t <- 1
  while (max(A %*% (x + t * v)) >= 1 || max(abs(x + t * v)) >= 1) {
    t <- beta * t
  }
  
  # Ensure to update Ax only once per iteration
  new_x <- x + t * v
  new_Ax <- A %*% new_x
  
  while (-sum(log(1 - new_Ax)) - sum(log(1 + new_x)) 
"
