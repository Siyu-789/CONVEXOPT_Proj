{
  "hash": "7cd8eefc1238e54d07df1d6c420bfe2a",
  "result": {
    "engine": "knitr",
    "markdown": "# Gradient Method & Newton Method {.unnumbered}\n\n## Problem 9.30 (10 points)\n\n### a\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngamma <- 0.01\nbeta <- 0.5\nmax_i <- 5\neta <- 0.01\n\n# size\nm <- 20\nn <- 10\n\n# initialization\nx <- rep(0, n) \nA <- matrix(runif(m * n), nrow = m)\n\nAx <- A %*% x\n\n# storage\nvalues <- numeric(max_i)\nsteplen <- numeric(max_i)\nn_i <- numeric(max_i)\n\n# defination of function\nf <- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient <- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# for loop\nfor (i in 1:max_i) {\n  # f(x)\n  val <- f(x, Ax)\n  \n  # Df(x)\n  grad <- gradient(x, Ax, A)\n  \n  # store iteration values\n  n_i[i] <- i\n  values[i] <- val\n  \n  # stop statement: ||Df(x)||_2 \\leq \\eta\n  if (is.infinite(norm(grad, type = \"2\")) || is.na(norm(grad, type = \"2\"))) {\n    break\n  } else if (norm(grad, type = \"2\") < eta) {\n    values <- values[1:i]  \n    n_i <- n_i[1:i]\n    break\n  }\n  \n  # direction of gradient method\n  v <- -grad\n  \n  # backtracking line search: loop for t\n  t <- 1\n  \n  while (max(A %*% (x + t * v)) >= 1 || max(x + t * v) >= 1 || min(x + t * v) <= -1) {\n    t <- beta * t\n  }\n  \n  # Ensure to update Ax only once per iteration\n  new_x <- x + t * v\n  new_Ax <- A %*% new_x\n  \n  # Corrected position of the while loop condition check\n  while (f(new_x, new_Ax) > val + gamma * t * crossprod(grad, v)) {\n    t <- beta * t\n    new_x <- x + t * v\n    new_Ax <- A %*% new_x\n  }\n  \n  # iteration\n  x <- new_x\n  Ax <- new_Ax\n  \n  # store iteration step length\n  steplen[i] <- t\n  \n  # Corrected progress message variable\n  cat(\"i:\", i, \"Value:\", val,\"Step Length:\", t, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ni: 1 Value: 0 Step Length: 0.0625 \ni: 2 Value: -22.20123 Step Length: 1 \ni: 3 Value: -22.93227 Step Length: 8.881784e-16 \ni: 4 Value: -22.93227 Step Length: 4.440892e-16 \ni: 5 Value: -22.93227 Step Length: 1.776357e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Gradient Descent')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Gradient Descent')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# different size\ngamma <- 0.01\nbeta <- 0.5\nmax_i <- 5\neta <- 0.01\n\n# size\nm <- 20\nn <- 10\n\n# initialization\nx <- rep(0, n) \nA <- matrix(runif(m * n), nrow = m)\n\nAx <- A %*% x\n\n# storage\nvalues <- numeric(max_i)\nsteplen <- numeric(max_i)\n\n# defination of function\nf <- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient <- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# for loop\nfor (i in 1:max_i) {\n  # f(x)\n  val <- f(x, Ax)\n  \n  # Df(x)\n  grad <- gradient(x, Ax, A)\n  \n  # store iteration values\n  values[i] <- val\n  \n  # stop statement: ||Df(x)||_2 \\leq \\eta\n  if (is.infinite(norm(grad, type = \"2\")) || is.na(norm(grad, type = \"2\"))) {\n    break\n  } else if (norm(grad, type = \"2\") < eta) {\n    values <- values[1:i]  \n    n_i <- n_i[1:i]\n    break\n  }\n  \n  # direction of gradient method\n  v <- -grad\n  \n  # backtracking line search: loop for t\n  t <- 1\n  \n  while (max(A %*% (x + t * v)) >= 1 || max(x + t * v) >= 1 || min(x + t * v) <= -1) {\n    t <- beta * t\n  }\n  \n  # Ensure to update Ax only once per iteration\n  new_x <- x + t * v\n  new_Ax <- A %*% new_x\n  \n  # Corrected position of the while loop condition check\n  while (f(new_x, new_Ax) > val + gamma * t * crossprod(grad, v)) {\n    t <- beta * t\n    new_x <- x + t * v\n    new_Ax <- A %*% new_x\n  }\n  \n  # iteration\n  x <- new_x\n  Ax <- new_Ax\n  \n  # store iteration step length\n  steplen[i] <- t\n  \n  # Corrected progress message variable\n  cat(\"i:\", i, \"Value:\", val,\"Step Length:\", t, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ni: 1 Value: 0 Step Length: 0.0625 \ni: 2 Value: -21.58727 Step Length: 1 \ni: 3 Value: -21.70757 Step Length: 0.0625 \ni: 4 Value: -21.90264 Step Length: 7.105427e-15 \ni: 5 Value: -21.90264 Step Length: 1.776357e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Gradient Descent')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Gradient Descent')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n### b\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngamma <- 0.01\nbeta <- 0.5\nmax_i <- 5\neta <- 0.01\n\n# size\nm <- 10\nn <- 10\n\n# initialization\nx <- rep(0, n) \nA <- matrix(runif(m * n), nrow = m)\n\nAx <- A %*% x\n\n# defination of function\nf <- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient <- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess <- function(x, Ax, A) {\n  d <- 1 / ((1 - Ax)^2)\n  diag_d <- diag(as.vector(d))\n  H <- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues <- numeric(max_i)\nsteplen <- numeric(max_i)\n\n\n# for loop\nfor (i in 1:max_i) {\n  Ax <- A %*% x\n  # f(x)\n  val <- f(x, Ax)\n  \n  # Df(x)\n  grad <- gradient(x, Ax, A)\n\n  # Hessian\n  H <- hess(x, Ax, A)\n  \n  # v and <t,v>\n  v <- solve(H, -grad)\n  fprime <- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) < eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t <- 1\n  while (max(A %*% (x + t * v)) >= 1 || max(x + t * v) >= 1 || min(x + t * v) <= -1) {\n    t <- beta * t\n  }\n  \n  new_x <- x + t * v\n  new_Ax <- A %*% new_x\n  \n  while (f(new_x, new_Ax) > val + gamma * t * fprime) {\n    t <- beta * t\n    new_x <- x + t * v\n    new_Ax <- A %*% new_x\n  }\n  \n  # iteration\n  x <- new_x\n  Ax <- new_Ax\n  \n  # store iteration step length\n  steplen[i] <- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -3.671415 Step Length: 0.0009765625 \ni: 3 Value: -3.672162 Step Length: 7.105427e-15 \ni: 4 Value: -3.672162 Step Length: 1.387779e-17 \ni: 5 Value: -3.672162 Step Length: 3.552714e-15 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(steplen, type = 'b', col = 'red', xlab = 'Iteration', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngamma <- 0.01\nbeta <- 0.5\nmax_i <- 5\neta <- 0.01\n\n# size\nm <- 200\nn <- 100\n\n# initialization\nx <- rep(0, n) \nA <- matrix(runif(m * n), nrow = m)\n\nAx <- A %*% x\n\n# defination of function\nf <- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient <- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess <- function(x, Ax, A) {\n  d <- 1 / ((1 - Ax)^2)\n  diag_d <- diag(as.vector(d))\n  H <- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues <- numeric(max_i)\nsteplen <- numeric(max_i)\n\n\n# for loop\nfor (i in 1:max_i) {\n  Ax <- A %*% x\n  # f(x)\n  val <- f(x, Ax)\n  \n  # Df(x)\n  grad <- gradient(x, Ax, A)\n\n  # Hessian\n  H <- hess(x, Ax, A)\n  \n  # v and <t,v>\n  v <- solve(H, -grad)\n  fprime <- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) < eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t <- 1\n  while (max(A %*% (x + t * v)) >= 1 || max(x + t * v) >= 1 || min(x + t * v) <= -1) {\n    t <- beta * t\n  }\n  \n  new_x <- x + t * v\n  new_Ax <- A %*% new_x\n  \n  while (f(new_x, new_Ax) > val + gamma * t * fprime) {\n    t <- beta * t\n    new_x <- x + t * v\n    new_Ax <- A %*% new_x\n  }\n  \n  # iteration\n  x <- new_x\n  Ax <- new_Ax\n  \n  # store iteration step length\n  steplen[i] <- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -136.2967 Step Length: 1 \ni: 3 Value: -253.5493 Step Length: 0.25 \ni: 4 Value: -268.9881 Step Length: 1.084202e-19 \ni: 5 Value: -268.9881 Step Length: 1.084202e-19 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(steplen, type = 'b', col = 'red', xlab = 'Iteration', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\n## Problem 9.31 (10 points)\n\n### a\n\n::: {.cell}\n\n```{.r .cell-code}\ngamma <- 0.01\nbeta <- 0.5\nmax_i <- 5\neta <- 0.01\nN=2\n\n# size\nm <- 10\nn <- 10\n\n# initialization\nx <- rep(0, n) \nA <- matrix(runif(m * n), nrow = m)\n\nAx <- A %*% x\n\n# defination of function\nf <- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient <- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess <- function(x, Ax, A) {\n  d <- 1 / ((1 - Ax)^2)\n  diag_d <- diag(as.vector(d))\n  H <- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues <- numeric(max_i)\nsteplen <- numeric(max_i)\nhessian_counter <- N\n\n# for loop\nfor (i in 1:max_i) {\n  Ax <- A %*% x\n  # f(x)\n  val <- f(x, Ax)\n  \n  # Df(x)\n  grad <- gradient(x, Ax, A)\n\n  # Hessian\n  if (hessian_counter >= N) {\n    H <- hess(x, Ax, A)\n    hessian_counter <- 0 \n  }\n  \n  # v and <t,v>\n  v <- solve(H, -grad)\n  fprime <- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) < eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t <- 1\n  while (max(A %*% (x + t * v)) >= 1 || max(x + t * v) >= 1 || min(x + t * v) <= -1) {\n    t <- beta * t\n  }\n  \n  new_x <- x + t * v\n  new_Ax <- A %*% new_x\n  \n  while (f(new_x, new_Ax) > val + gamma * t * fprime) {\n    t <- beta * t\n    new_x <- x + t * v\n    new_Ax <- A %*% new_x\n  }\n  \n  # iteration\n  x <- new_x\n  Ax <- new_Ax\n  \n  # store iteration step length\n  steplen[i] <- t\n\n  # Update Hessian counter\n  hessian_counter <- hessian_counter + 1\n\n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -4.321432 Step Length: 1 \ni: 3 Value: -5.375923 Step Length: 1.110223e-16 \ni: 4 Value: -5.375923 Step Length: 6.938894e-18 \ni: 5 Value: -5.375923 Step Length: 3.469447e-18 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n### b\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngamma <- 0.01\nbeta <- 0.5\nmax_i <- 5\neta <- 0.01\nN=2\n\n# size\nm <- 10\nn <- 10\n\n# initialization\nx <- rep(0, n) \nA <- matrix(runif(m * n), nrow = m)\n\nAx <- A %*% x\n\n# defination of function\nf <- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient <- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# Modified\nhess_diag <- function(x, Ax, A) {\n  d <- 1 / ((1 - Ax)^2)\n  # Diagonal Hessian approximation\n  diag_H <- 1 / ((1 - x)^2) + 1 / ((1 + x)^2) + (t(A) * A) %*% d\n  H_diag <- diag(diag(diag_H))\n  return(H_diag)\n}\n\n\n# storage\nvalues <- numeric(max_i)\nsteplen <- numeric(max_i)\n\n# for loop\nfor (i in 1:max_i) {\n  Ax <- A %*% x\n  # f(x)\n  val <- f(x, Ax)\n  \n  # Df(x)\n  grad <- gradient(x, Ax, A)\n\n  # Hessian diagonal approximation\n  H_diag <- hess_diag(x, Ax, A)\n  \n  # Inverse of diag_H approximation\n  H_inv <- 1 / diag(H_diag)\n\n  # v and <t,v>\n  v <- H_inv * (-grad)\n  fprime <- sum(grad * v)\n  \n  # stop statement: \\lambda^2 / 2\n  if (abs(fprime / 2) < eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t <- 1\n  while (max(A %*% (x + t * v)) >= 1 || max(x + t * v) >= 1 || min(x + t * v) <= -1) {\n    t <- beta * t\n  }\n  \n  new_x <- x + t * v\n  new_Ax <- A %*% new_x\n  \n  while (f(new_x, new_Ax) > val + gamma * t * fprime) {\n    t <- beta * t\n    new_x <- x + t * v\n    new_Ax <- A %*% new_x\n  }\n  \n  # iteration\n  x <- new_x\n  Ax <- new_Ax\n  \n  # store iteration step length\n  steplen[i] <- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in H_inv * (-grad): longer object length is not a multiple of shorter\nobject length\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\ni: 1 Value: 0 Step Length: 0.125 \ni: 2 Value: -9.43772 Step Length: 2.775558e-17 \ni: 3 Value: -9.43772 Step Length: 2.775558e-17 \ni: 4 Value: -9.43772 Step Length: 2.775558e-17 \ni: 5 Value: -9.43772 Step Length: 2.775558e-17 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n```\n\n::: {.cell-output-display}\n![](GradientMethod_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::",
    "supporting": [
      "GradientMethod_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}