[
  {
    "objectID": "GradientMethod.html",
    "href": "GradientMethod.html",
    "title": "1  Gradient Method & Newton Method",
    "section": "",
    "text": "1.1 Problem 9.30 (10 points)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Gradient Method & Newton Method</span>"
    ]
  },
  {
    "objectID": "GradientMethod.html#problem-9.30-10-points",
    "href": "GradientMethod.html#problem-9.30-10-points",
    "title": "1  Gradient Method & Newton Method",
    "section": "",
    "text": "1.1.1 a\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 20\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\nn_i &lt;- numeric(max_i)\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# for loop\nfor (i in 1:max_i) {\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n  \n  # store iteration values\n  n_i[i] &lt;- i\n  values[i] &lt;- val\n  \n  # stop statement: ||Df(x)||_2 \\leq \\eta\n  if (is.infinite(norm(grad, type = \"2\")) || is.na(norm(grad, type = \"2\"))) {\n    break\n  } else if (norm(grad, type = \"2\") &lt; eta) {\n    values &lt;- values[1:i]  \n    n_i &lt;- n_i[1:i]\n    break\n  }\n  \n  # direction of gradient method\n  v &lt;- -grad\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  \n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  # Ensure to update Ax only once per iteration\n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  # Corrected position of the while loop condition check\n  while (f(new_x, new_Ax) &gt; val + gamma * t * crossprod(grad, v)) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  # Corrected progress message variable\n  cat(\"i:\", i, \"Value:\", val,\"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 0.0625 \ni: 2 Value: -21.93311 Step Length: 1 \ni: 3 Value: -22.50242 Step Length: 0.03125 \ni: 4 Value: -22.67107 Step Length: 8.881784e-16 \ni: 5 Value: -22.67107 Step Length: 1.776357e-15 \n\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Gradient Descent')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Gradient Descent')\n\n\n\n\n\n\n\n\n\n# different size\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 20\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# for loop\nfor (i in 1:max_i) {\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n  \n  # store iteration values\n  values[i] &lt;- val\n  \n  # stop statement: ||Df(x)||_2 \\leq \\eta\n  if (is.infinite(norm(grad, type = \"2\")) || is.na(norm(grad, type = \"2\"))) {\n    break\n  } else if (norm(grad, type = \"2\") &lt; eta) {\n    values &lt;- values[1:i]  \n    n_i &lt;- n_i[1:i]\n    break\n  }\n  \n  # direction of gradient method\n  v &lt;- -grad\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  \n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  # Ensure to update Ax only once per iteration\n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  # Corrected position of the while loop condition check\n  while (f(new_x, new_Ax) &gt; val + gamma * t * crossprod(grad, v)) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  # Corrected progress message variable\n  cat(\"i:\", i, \"Value:\", val,\"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 0.0625 \ni: 2 Value: -22.89707 Step Length: 2.842171e-14 \ni: 3 Value: -22.89707 Step Length: 1.776357e-15 \ni: 4 Value: -22.89707 Step Length: 8.881784e-16 \ni: 5 Value: -22.89707 Step Length: 2.220446e-16 \n\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Gradient Descent')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Gradient Descent')\n\n\n\n\n\n\n\n\n\n\n1.1.2 b\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 10\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  diag_d &lt;- diag(as.vector(d))\n  H &lt;- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian\n  H &lt;- hess(x, Ax, A)\n  \n  # v and &lt;t,v&gt;\n  v &lt;- solve(H, -grad)\n  fprime &lt;- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -3.936196 Step Length: 0.00390625 \ni: 3 Value: -3.936899 Step Length: 8.673617e-19 \ni: 4 Value: -3.936899 Step Length: 8.673617e-19 \ni: 5 Value: -3.936899 Step Length: 8.673617e-19 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'Iteration', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n\n\n\n\n\n\n\n\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 200\nn &lt;- 100\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  diag_d &lt;- diag(as.vector(d))\n  H &lt;- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian\n  H &lt;- hess(x, Ax, A)\n  \n  # v and &lt;t,v&gt;\n  v &lt;- solve(H, -grad)\n  fprime &lt;- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -136.268 Step Length: 1 \ni: 3 Value: -252.9817 Step Length: 0.25 \ni: 4 Value: -267.7972 Step Length: 1.734723e-18 \ni: 5 Value: -267.7972 Step Length: 1.734723e-18 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'Iteration', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Gradient Method & Newton Method</span>"
    ]
  },
  {
    "objectID": "GradientMethod.html#problem-9.31-10-points",
    "href": "GradientMethod.html#problem-9.31-10-points",
    "title": "1  Gradient Method & Newton Method",
    "section": "1.2 Problem 9.31 (10 points)",
    "text": "1.2 Problem 9.31 (10 points)\n\n1.2.1 a\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\nN=2\n\n# size\nm &lt;- 10\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  diag_d &lt;- diag(as.vector(d))\n  H &lt;- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\nhessian_counter &lt;- N\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian\n  if (hessian_counter &gt;= N) {\n    H &lt;- hess(x, Ax, A)\n    hessian_counter &lt;- 0 \n  }\n  \n  # v and &lt;t,v&gt;\n  v &lt;- solve(H, -grad)\n  fprime &lt;- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n\n  # Update Hessian counter\n  hessian_counter &lt;- hessian_counter + 1\n\n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -4.030226 Step Length: 1 \ni: 3 Value: -4.950666 Step Length: 5.551115e-17 \ni: 4 Value: -4.950666 Step Length: 5.551115e-17 \ni: 5 Value: -4.950666 Step Length: 8.673617e-19 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n\n\n\n\n\n\n\n\n\n\n1.2.2 b\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\nN=2\n\n# size\nm &lt;- 10\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# Modified\nhess_diag &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  # Diagonal Hessian approximation\n  diag_H &lt;- 1 / ((1 - x)^2) + 1 / ((1 + x)^2) + (t(A) * A) %*% d\n  H_diag &lt;- diag(diag(diag_H))\n  return(H_diag)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian diagonal approximation\n  H_diag &lt;- hess_diag(x, Ax, A)\n  \n  # Inverse of diag_H approximation\n  H_inv &lt;- 1 / diag(H_diag)\n\n  # v and &lt;t,v&gt;\n  v &lt;- H_inv * (-grad)\n  fprime &lt;- sum(grad * v)\n  \n  # stop statement: \\lambda^2 / 2\n  if (abs(fprime / 2) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\nWarning in H_inv * (-grad): longer object length is not a multiple of shorter\nobject length\n\n\ni: 1 Value: 0 Step Length: 0.125 \ni: 2 Value: -9.584415 Step Length: 1.110223e-16 \ni: 3 Value: -9.584415 Step Length: 2.775558e-17 \ni: 4 Value: -9.584415 Step Length: 2.775558e-17 \ni: 5 Value: -9.584415 Step Length: 2.775558e-17 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Gradient Method & Newton Method</span>"
    ]
  }
]