<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Siyu Chen">

<title>Convex Optimization - 2&nbsp; Report</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./project_code.html" rel="next">
<link href="./proposal.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./proposal.html">Term Project</a></li><li class="breadcrumb-item"><a href="./Project.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Report</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Convex Optimization</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Term Project</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Project.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Report</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project_code.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">project_code.html</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Homework</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GradientMethod.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gradient Method &amp; Newton Method</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3</span> Introduction</a></li>
  <li><a href="#lasso-least-absolute-shrinkage-and-selection-operator" id="toc-lasso-least-absolute-shrinkage-and-selection-operator" class="nav-link" data-scroll-target="#lasso-least-absolute-shrinkage-and-selection-operator"><span class="header-section-number">4</span> LASSO (Least Absolute Shrinkage and Selection Operator)</a>
  <ul class="collapse">
  <li><a href="#background-of-lasso" id="toc-background-of-lasso" class="nav-link" data-scroll-target="#background-of-lasso"><span class="header-section-number">4.1</span> Background of Lasso</a></li>
  <li><a href="#convexity-and-differentiability-in-lasso-problem" id="toc-convexity-and-differentiability-in-lasso-problem" class="nav-link" data-scroll-target="#convexity-and-differentiability-in-lasso-problem"><span class="header-section-number">4.2</span> Convexity and Differentiability in Lasso Problem</a></li>
  <li><a href="#challenges-of-gradient-descent-method-in-lasso-problem" id="toc-challenges-of-gradient-descent-method-in-lasso-problem" class="nav-link" data-scroll-target="#challenges-of-gradient-descent-method-in-lasso-problem"><span class="header-section-number">4.3</span> Challenges of Gradient Descent Method in Lasso Problem</a></li>
  </ul></li>
  <li><a href="#subgradient-method-in-lasso-problem" id="toc-subgradient-method-in-lasso-problem" class="nav-link" data-scroll-target="#subgradient-method-in-lasso-problem"><span class="header-section-number">5</span> Subgradient Method in Lasso Problem</a>
  <ul class="collapse">
  <li><a href="#materials-of-subgradient" id="toc-materials-of-subgradient" class="nav-link" data-scroll-target="#materials-of-subgradient"><span class="header-section-number">5.1</span> Materials of Subgradient</a></li>
  <li><a href="#subgradient-in-lasso" id="toc-subgradient-in-lasso" class="nav-link" data-scroll-target="#subgradient-in-lasso"><span class="header-section-number">5.2</span> Subgradient in Lasso</a></li>
  <li><a href="#subgradient-descent-algorithm" id="toc-subgradient-descent-algorithm" class="nav-link" data-scroll-target="#subgradient-descent-algorithm"><span class="header-section-number">5.3</span> Subgradient Descent Algorithm</a></li>
  </ul></li>
  <li><a href="#coordinate-descent-method-in-lasso-problem" id="toc-coordinate-descent-method-in-lasso-problem" class="nav-link" data-scroll-target="#coordinate-descent-method-in-lasso-problem"><span class="header-section-number">6</span> Coordinate Descent Method in Lasso Problem</a>
  <ul class="collapse">
  <li><a href="#coordinate-descent-method-in-lasso" id="toc-coordinate-descent-method-in-lasso" class="nav-link" data-scroll-target="#coordinate-descent-method-in-lasso"><span class="header-section-number">6.1</span> Coordinate Descent Method in Lasso</a></li>
  <li><a href="#coordinate-gradient-algorithm" id="toc-coordinate-gradient-algorithm" class="nav-link" data-scroll-target="#coordinate-gradient-algorithm"><span class="header-section-number">6.2</span> Coordinate Gradient Algorithm</a></li>
  </ul></li>
  <li><a href="#proximal-gradient-method-in-lasso-problem" id="toc-proximal-gradient-method-in-lasso-problem" class="nav-link" data-scroll-target="#proximal-gradient-method-in-lasso-problem"><span class="header-section-number">7</span> Proximal Gradient Method in Lasso Problem</a>
  <ul class="collapse">
  <li><a href="#materials-of-proximal-gradient" id="toc-materials-of-proximal-gradient" class="nav-link" data-scroll-target="#materials-of-proximal-gradient"><span class="header-section-number">7.1</span> Materials of Proximal Gradient</a></li>
  <li><a href="#detailed-derivation-of-proximal-gradient-descent-for-lasso" id="toc-detailed-derivation-of-proximal-gradient-descent-for-lasso" class="nav-link" data-scroll-target="#detailed-derivation-of-proximal-gradient-descent-for-lasso"><span class="header-section-number">7.2</span> Detailed Derivation of Proximal Gradient Descent for Lasso</a></li>
  <li><a href="#proximal-gradient-algorithm" id="toc-proximal-gradient-algorithm" class="nav-link" data-scroll-target="#proximal-gradient-algorithm"><span class="header-section-number">7.3</span> Proximal Gradient Algorithm</a></li>
  </ul></li>
  <li><a href="#evaluation-of-performance-of-algorithms-for-lasso" id="toc-evaluation-of-performance-of-algorithms-for-lasso" class="nav-link" data-scroll-target="#evaluation-of-performance-of-algorithms-for-lasso"><span class="header-section-number">8</span> Evaluation of performance of Algorithms for Lasso</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">9</span> Conclusion</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference"><span class="header-section-number">10</span> Reference</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./proposal.html">Term Project</a></li><li class="breadcrumb-item"><a href="./Project.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Report</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Report</span></h1>
<p class="subtitle lead">Report of Convex Optimization Term Project | Due Date: April 29th, 2024 Professor: Waheed U. Bajwa</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Siyu Chen </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>This report examines the efficiency and effectiveness of three different optimization algorithms—Subgradient, Coordinate Descent, and Proximal Gradient methods—in solving LASSO (Least Absolute Shrinkage and Selection Operator) optimization problems. LASSO is a popular method in regression analysis that integrates variable selection and regularization to improve the predictability and interpretability of statistical models. Despite its advantages, the LASSO’s non-differentiable nature due to its regularization term poses unique challenges. Our study focuses on comparing these algorithms to determine their suitability for various types of LASSO problems, evaluating their practical performance, advantages, and limitations.</p>
  </div>
</div>


</header>


<section id="introduction" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Introduction</h1>
<p>The Least Absolute Shrinkage and Selection Operator (Lasso) method is a cornerstone of regression analysis, known for its ability to both select variables and regularize data to enhance model interpretability and prediction accuracy. Lasso optimization involves solving a convex optimization problem that is not necessarily differentiable due to the absolute value in its regularization term. This term project explores three distinct optimization algorithms that address these challenges: the Subgradient method, Coordinate Descent, and the Proximal Gradient method. Through a comparative analysis, this report aims to highlight the practical performances, strengths, and weaknesses of each algorithm within the context of Lasso optimization.</p>
</section>
<section id="lasso-least-absolute-shrinkage-and-selection-operator" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> LASSO (Least Absolute Shrinkage and Selection Operator)</h1>
<section id="background-of-lasso" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="background-of-lasso"><span class="header-section-number">4.1</span> Background of Lasso</h2>
<p>In 1996, Robert Tibshirani first introduced the Lasso, a method now commonly encountered in various fields. The Lasso technique is designed to enhance the prediction accuracy and interpretability of the statistical models by altering the regularization process.</p>
<p>The form of Lasso is given by the following optimization problem:</p>
<p><span id="eq-lasso1"><span class="math display">\[
\begin{align}
\min_{\beta} \left\{ \left\| y - X\beta \right\|_2^2 \right\} \\
\text{subject to} \quad \left\| \beta \right\|_1 \leq s
\end{align}
\tag{4.1}\]</span></span></p>
<p>Here, <span class="math inline">\(y \in \mathbb{R}^n\)</span> represents the response vector, <span class="math inline">\(X \in \mathbb{R}^{n*p}\)</span> is the matrix of predictors, <span class="math inline">\(\beta \in \mathbb{R}^{p}\)</span> denotes the coefficient vector and <span class="math inline">\(s\in \mathbb{R}_+\)</span> determining the degree of regularization</p>
<p>See <a href="#fig-lasso" class="quarto-xref">Figure&nbsp;<span>4.1</span></a> illustrates the simple situation of p=2.</p>
<div id="fig-lasso" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lasso-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/paste-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lasso-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Contours of the error and constraint functions for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regions, <span class="math inline">\(|β_1| + |β_2| ≤ s\)</span> and <span class="math inline">\(β_1^2 + β_2^2 ≤ s\)</span>, while the red ellipses are the contours ofthe RSS.
</figcaption>
</figure>
</div>
<p><a href="#eq-lasso1" class="quarto-xref">Equation&nbsp;<span>4.1</span></a> is equivalent to the following unconstrained optimization problem by logrange dual function.</p>
<p><span id="eq-lasso2"><span class="math display">\[
\begin{equation}
\min_{\beta} \left\{\left\| y - X\beta \right\|_2^2 + \lambda \left\| \beta \right\|_1 \right\}
\end{equation}
\tag{4.2}\]</span></span></p>
<p>Here, <span class="math inline">\(\lambda \in R^+\)</span>. The choice of 𝜆 controls the trade-off between the sparsity of the model 𝛽 and the fit of the model.</p>
</section>
<section id="convexity-and-differentiability-in-lasso-problem" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="convexity-and-differentiability-in-lasso-problem"><span class="header-section-number">4.2</span> Convexity and Differentiability in Lasso Problem</h2>
<p>To elucidate the optimization problem presented in <a href="#eq-lasso2" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>, we denote the objective function by</p>
<p><span class="math display">\[
\begin{align}
f(\beta) &amp;= g(\beta) + h(\beta), \\
\text{where } g(\beta) &amp;= \left\| y - X\beta \right\|_2^2,\
h(\beta) = \lambda \left\| \beta \right\|_1
\end{align}
\]</span></p>
<p>The Lasso optimization problem can then be rewritten from <a href="#eq-lasso2" class="quarto-xref">Equation&nbsp;<span>4.2</span></a> as</p>
<p><span class="math display">\[
min_{\beta \in R^p}f(\beta)
\]</span></p>
<p>where the domain of the solution space is explicitly specified as <span class="math inline">\(R^p\)</span>.This reformulation aligns with the general optimization framework <span class="math inline">\(min_{x\in C}f(x)\)</span> and facilitates the subsequent analysis.</p>
<p>In the context of convexity, the objective function <span class="math inline">\(f(\beta)\)</span> is established as convex. This is attributed to it being the non-negative sum of <span class="math inline">\(g(\beta)\)</span>, which itself is convex by virtue of being the composition of an affine function and the <span class="math inline">\(L_2\)</span> norm, and <span class="math inline">\(h(\beta)\)</span>, which is the <span class="math inline">\(L_1\)</span> norm.</p>
<p>With regard to differentiability, the function <span class="math inline">\(f(\beta)\)</span> becomes non-differentiable because the term <span class="math inline">\(h(\beta)\)</span> is non-differentiable at the point <span class="math inline">\(\beta=0\)</span> despite the differentiability of <span class="math inline">\(g(\beta)\)</span>.</p>
</section>
<section id="challenges-of-gradient-descent-method-in-lasso-problem" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="challenges-of-gradient-descent-method-in-lasso-problem"><span class="header-section-number">4.3</span> Challenges of Gradient Descent Method in Lasso Problem</h2>
<p>In unconstrained optimization problems as Boyd Stephen said <span class="citation" data-cites="textbook">see <a href="proposal.html#ref-textbook" role="doc-biblioref">[1, Ch. 9]</a></span>:</p>
<p><span id="eq-GD"><span class="math display">\[
min_{x\in C}f(x)
\tag{4.3}\]</span></span></p>
<p>where <span class="math inline">\(f(x)\)</span> is a convex function, and <span class="math inline">\(C=domf\)</span> represents a convex set. When <span class="math inline">\(f(x)\)</span> is differentiable, the optimality condition is defined as: <span id="eq-GD-optimal"><span class="math display">\[
f(x^*) = \min_{x} f(x) \Leftrightarrow \nabla f(x^*)=0
\tag{4.4}\]</span></span></p>
<p>However, LASSO is non-differentiable. To solve this problem, the concept of a subgradient will be introduced later in this report, which extends the concept of a gradient to include non-differentiable functions.</p>
</section>
</section>
<section id="subgradient-method-in-lasso-problem" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Subgradient Method in Lasso Problem</h1>
<section id="materials-of-subgradient" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="materials-of-subgradient"><span class="header-section-number">5.1</span> Materials of Subgradient</h2>
<div id="thm-convex-differentiable" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1</strong></span> If a function <span class="math inline">\(f:\mathbb{R}^n \to \mathbb{R}\)</span> is both differentiable and convex, then for any two points <span class="math inline">\(x, y\)</span> within a convex set <span class="math inline">\(C \subseteq \mathbb{R}^n\)</span> , the following inequality holds:</p>
<p><span id="eq-convex-differentiable"><span class="math display">\[
f(y) \geq f(x)+\nabla f(x)^\top (y - x)
\tag{5.1}\]</span></span></p>
</div>
<p>The porperty of convexity shown in <a href="#thm-convex-differentiable" class="quarto-xref">Theorem&nbsp;<span>5.1</span></a> leads to the definition of subgradient as following:</p>
<div id="thm-convex-nondifferentiable" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.2</strong></span> For a function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, a vector <span class="math inline">\(g \in \mathbb{R}^n\)</span> is called a <em>subgradient</em> of <span class="math inline">\(f\)</span> at <span class="math inline">\(x \in dom f\)</span> if for all <span class="math inline">\(x \in \text{dom} f\)</span>,</p>
<p><span class="math display">\[
f(x) \geq f(x_0) + g^\top (x - x_0)
\]</span></p>
</div>
<p>If f is convex and differentiable, then its gradient at x is a subgradient. However a subgradient can exist even when f is not differentiable at x. In such cases, the subdifferential at x consists of all vectors that satisfy the definition of a subgradient at the non-differentiable point. These vectors form a set which is a convex set that describes directions in which the value of the function 𝑓 decrease.</p>
<div id="thm-subgradient-definition2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.3</strong></span> The <em>subdifferential</em> of a function <span class="math inline">\(f\)</span> at a point <span class="math inline">\(x_0\)</span> is defined as the set of subgradients of <span class="math inline">\(f\)</span> at any <span class="math inline">\(x\in dom f\)</span>:</p>
<p><span class="math display">\[
\partial f(x) = \{ g : g \text{ is a subgradient of } f \text{ at } x \in dom f\}
\]</span></p>
</div>
<p>Then the optimal condition is defined as: <span id="eq-subG-optimal"><span class="math display">\[
f(x^*) = \min_{x} f(x) \Leftrightarrow 0 \in \partial f(x^*)
\tag{5.2}\]</span></span></p>
</section>
<section id="subgradient-in-lasso" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="subgradient-in-lasso"><span class="header-section-number">5.2</span> Subgradient in Lasso</h2>
<p>Using <a href="#eq-subG-optimal" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>, the subgradient of f(x) is:</p>
<p><span id="eq-subG-gradient"><span class="math display">\[
\partial f(\beta) = -2X^T \left( y - X\beta \right) + \lambda \, \partial \left\| \beta \right\|_1
\tag{5.3}\]</span></span></p>
<p>And <span class="math inline">\(\partial \left\| \beta \right\|_1\)</span> is:</p>
<p><span id="eq-subG-gradient2"><span class="math display">\[
\partial \left\| \beta \right\|_1 =
\begin{cases}
1, &amp; \beta &gt; 0 \\
[-1,1], &amp; \beta=0 \\
-1, &amp; \beta &lt; 0
\end{cases}
\tag{5.4}\]</span></span></p>
<p>Therefore, The subgradient of the function can be characterized as:</p>
<p><span class="math display">\[
\begin{cases}
2X_j^\top (y - X\beta) = \lambda \operatorname{sign}(\beta_j) &amp; \text{if } \beta_j \neq 0 \\
|2(X_j)^\top (y - X\beta)| \leq \lambda &amp; \text{if } \beta_j = 0
\end{cases}
\]</span></p>
<p>Here, <span class="math inline">\(sign(\beta)\)</span> is:</p>
<p><span class="math display">\[
sign(\beta) =
\begin{cases}
1, &amp; \beta &gt; 0 \\
-1, &amp; \beta &lt; 0
\end{cases}
\]</span></p>
</section>
<section id="subgradient-descent-algorithm" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="subgradient-descent-algorithm"><span class="header-section-number">5.3</span> Subgradient Descent Algorithm</h2>
<p>In the iterative process, a single iteration step involves a forward movement from the current position along a certain direction. Specifically, during each iteration, based on the current position, a movement is made towards the negative gradient direction to find a new position, thus forming an iterative sequence. The iteration rule is:</p>
<p><span id="eq-sub-iteration"><span class="math display">\[
x^k = x^{k-1} - t_k g^k, \quad g^k \in \partial f(x^{k-1})
\tag{5.5}\]</span></span></p>
<p>During the iterative optimization process, each step is a finite improvement, which does not guarantee the overall convergence of the iterative sequence by itself. However, by selecting the best solution from all the iterations thus far, convergence towards an optimal solution encountered during the iteration process is achieved. The best solution obtained from all the iterations can be characterized as:</p>
<p><span class="math display">\[
f(x^k_{\text{best}}) = \min \{ f(x^i) \}_{i=0,...,k}
\]</span></p>
<p>The complete subgradient descent algorithm process is as follows.</p>
<ol type="1">
<li>Set k=0 and initial point <span class="math inline">\(\beta_1=\beta_2=...=\beta_p=0\)</span> and fixed step size t=1</li>
</ol>
<p>Repeat</p>
<ol start="2" type="1">
<li><p>Compute the predicted values <span class="math inline">\(\hat Y=X\beta\)</span></p></li>
<li><p>Calculate gradients by <a href="#eq-subG-gradient" class="quarto-xref">Equation&nbsp;<span>5.3</span></a></p></li>
<li><p>Updata parameter by <a href="#eq-sub-iteration" class="quarto-xref">Equation&nbsp;<span>5.5</span></a></p></li>
</ol>
<p>until termination test satisfied.</p>
</section>
</section>
<section id="coordinate-descent-method-in-lasso-problem" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Coordinate Descent Method in Lasso Problem</h1>
<p>The Coordinate Descent Method optimizes one variable at a time by fixing all other variables. Its iterative nature involves cycling through each variable and performing optimization until convergence.</p>
<p>One of the primary benefits of the Coordinate Descent Method is its computational simplicity and efficiency. By breaking down the high-dimensional optimization problem into a series of one-dimensional problems, it becomes easier to implement and solve, even with very large datasets. The method also lends itself well to parallelization, which can expedite the computation process considerably.</p>
<p>However, there are some notable limitations. The algorithm can experience a slow convergence rate if the variables in the dataset are highly correlated. In cases of non-convex optimization problems, the method may also fail to find the global minimum as it optimizes each coordinate in isolation, which may not reflect the overall direction of the true minimum.</p>
<section id="coordinate-descent-method-in-lasso" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="coordinate-descent-method-in-lasso"><span class="header-section-number">6.1</span> Coordinate Descent Method in Lasso</h2>
<p>When using the coordinate descent method for <a href="#eq-lasso2" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>, a key issue arises because the objective function is non-differentiable at <span class="math inline">\(x=0\)</span> due to the absolute value term that is included in the LASSO penalty. The coordinate descent approach addresses this by utilizing soft thredholding.</p>
<p>According to the subgradient condition denoted as <a href="#eq-subG-gradient" class="quarto-xref">Equation&nbsp;<span>5.3</span></a>,the formula can be rewritten in the form of <a href="#eq-subG-gradient3" class="quarto-xref">Equation&nbsp;<span>6.1</span></a> .This reformulation is crucial in the coordinate descent method, as it optimizes one dimension at a time while other dimensions remain fixed. Making the dimensions explicit is key for the later derivations.</p>
<p><span id="eq-subG-gradient3"><span class="math display">\[
\frac{\partial f(\beta)}{\partial \beta_k} = \sum_{i=1}^n-2x_{ik} \left( y_i - \sum_{j=1}^mx_{ij}\beta_j \right) + \lambda \, \frac{\partial \sum_{j=1}^m | \beta_k |}{\partial \beta_k}
\tag{6.1}\]</span></span></p>
<p>And <span class="math inline">\(\partial \left\| \beta \right\|_1\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial \sum_{j=1}^m | \beta_k |}{\partial \beta_k} =
\begin{cases}
1, &amp; \beta_k &gt; 0 \\
[-1,1], &amp; \beta_k=0 \\
-1, &amp; \beta_k &lt; 0
\end{cases}
\]</span></p>
<p>Proceeding with the derivation under the assumption that optimization is conducted along the k-th dimension, taking the partial derivative with respect to this particular dimension yields the following expression:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial f(\beta)}{\partial \beta_k} &amp;= \sum_{i=1}^n -2x_{ik} \left( y_i - \sum_{j=1}^m x_{ij}\beta_j \right) + \frac{\partial \sum_{j=1}^m | \beta_k |}{\partial \beta_k} \\
&amp;= \sum_{i=1}^n -2x_{ik} \left( y_i - \sum_{\substack{j=1 \\ j \neq k}}^m x_{ij}\beta_j - x_{ik}\beta_k \right) + \frac{\partial \sum_{j=1}^m | \beta_k |}{\partial \beta_k} \\
&amp;= -2 \sum_{i=1}^n x_{ik} \left( y_i - \sum_{\substack{j=1 \\ j \neq k}}^m x_{ij}\beta_j \right) + 2\beta_k \sum_{i=1}^n x_{ik}^2 + \frac{\partial \sum_{j=1}^m | \beta_k |}{\partial \beta_k} \\
&amp;= p_k + m_k\beta_k+\frac{\partial \sum_{j=1}^m | \beta_k |}{\partial \beta_k}
\end{aligned}
\]</span></p>
<p>Assuming that <span class="math inline">\(\beta_k\)</span> does not equal zero, we equate the partial derivative to zero to find the optimal value of <span class="math inline">\(\beta_k\)</span>, then</p>
<p><span class="math display">\[
\begin{aligned}
p_k + m_k\beta_k^*+\lambda sign(\beta_k^*)=0 \\
\Rightarrow \beta_k^* + \frac{1}{m_k} (p_k + \lambda \text{sign}(\beta_k^*)) = 0
\end{aligned}
\]</span></p>
<p>For <span class="math inline">\(\beta_k^*\)</span>, there are three cases to consider based on the value of p_k relative to <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(m_k \geq 0\)</span>, and it can be written to a solution of <span class="math inline">\(\beta_k^*\)</span> which is also called soft thredholding (<span class="math inline">\(S_{\lambda}(p_k)\)</span>)</p>
<p><span id="eq-cg-closedform"><span class="math display">\[
S_{\lambda}(p_k)=\beta_k^* =
\begin{cases}
-\frac{1}{m_k}(p_k - \lambda) &amp; \text{if } p_k &gt; \lambda, \\
-\frac{1}{m_k}(p_k + \lambda) &amp; \text{if } p_k &lt; -\lambda, \\
0 &amp; \text{if } -\lambda \leq p_k \leq \lambda.
\end{cases}
\tag{6.2}\]</span></span></p>
<p><a href="#eq-cg-closedform" class="quarto-xref">Equation&nbsp;<span>6.2</span></a> is a closed form of solution. Therefore, by applying soft thresholding, we can compute the value of <span class="math inline">\(p_k\)</span> and compare it directly with <span class="math inline">\(\lambda\)</span> to obtain the optimal value of <span class="math inline">\(\beta_k\)</span>. For instance, if the value of <span class="math inline">\(p_k\)</span> falls within the range of <span class="math inline">\(-\lambda\)</span> to <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(\beta_k^*\)</span> will be shrunk to zero.</p>
</section>
<section id="coordinate-gradient-algorithm" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="coordinate-gradient-algorithm"><span class="header-section-number">6.2</span> Coordinate Gradient Algorithm</h2>
<p>Here’s how it works in the context of coordinate descent for LASSO:</p>
<ol type="1">
<li>Set k=0 and choose <span class="math inline">\(x^0 \in \mathbb{R}^n\)</span>;</li>
</ol>
<p>Repeat</p>
<ol start="3" type="1">
<li><p>Compute <span class="math inline">\(p_k=-2 \sum_{i=1}^n x_{ik} \left( y_i - \sum_{\substack{j=1 \\ j \neq k}}^m x_{ij}\beta_j \right)\)</span></p></li>
<li><p>Compute <span class="math inline">\(m_k=2\sum_{i=1}^n x_{ik}^2\)</span></p></li>
<li><p>Set <span class="math inline">\(\beta_k=S_{\lambda}(p_k)\)</span></p></li>
<li><p>k=k+1</p></li>
</ol>
<p>until convergence or max number of iterations;</p>
</section>
</section>
<section id="proximal-gradient-method-in-lasso-problem" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Proximal Gradient Method in Lasso Problem</h1>
<section id="materials-of-proximal-gradient" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="materials-of-proximal-gradient"><span class="header-section-number">7.1</span> Materials of Proximal Gradient</h2>
<p>The proximal gradient method aims to minimize a composite function of the form:</p>
<p><span class="math display">\[
x = \arg \min_{x} g(x) + h(x)
\]</span></p>
<p>Here, g(x) is a differentiable convex function and h(x) is typically a non-differentiable convex function.</p>
<p>The proximal gradient method addresses this optimization problem by considering different characteristics of h(x):</p>
<ol type="1">
<li><p>If h(x)=0, the problem reduces to a standard gradient descent scenario.</p></li>
<li><p>If <span class="math inline">\(h(x)=I_c(x)\)</span>, where <span class="math inline">\(I_c(x)\)</span> is the indicator function for the set C, defined as</p></li>
</ol>
<p><span class="math display">\[
I_c=\begin{cases}
0, &amp; x \in C\\
\infty， &amp; x \notin C
\end{cases}
\]</span></p>
<p>then the project gradient descent can be employed to enforce the constraints defined by C.</p>
<ol start="3" type="1">
<li>If <span class="math inline">\(h(x)=\lambda \|x\|_1\)</span> then then the Iterative Shrinkage-Thresholding Algorithm (ISTA) can be utilized.</li>
</ol>
<p>In the context of the Lasso problem, as referenced by <a href="#eq-lasso2" class="quarto-xref">Equation&nbsp;<span>4.2</span></a>, we often encounter large matrix computations involving terms like <span class="math inline">\((X\top X)^{-1}\)</span>, which are computationally intensive. In the ISTA framework, we address this by considering a modified Lasso problem where the matrix X is omitted, focusing instead on a direct relationship between the predictors and the response variable.</p>
<p>The simplified Lasso problem, neglecting the matrix X, is then formulated as:</p>
<p><span id="eq-pgd-lassonoA"><span class="math display">\[
\begin{equation}
\min_{\beta} \left\{\left\| y - \beta \right\|_2^2 + \lambda \left\| \beta \right\|_1 \right\}
\end{equation}
\tag{7.1}\]</span></span></p>
<p>In this reformulation, the solution can leverage the soft thresholding operator <span class="math inline">\(S_{\lambda}(b)\)</span> to efficiently solve the optimization problem. Soft thresholding serves as the proximal operator in this setting, and is defined as:</p>
<p><span id="eq-pgd-softt"><span class="math display">\[
x^*==
\begin{cases}
b + \lambda, &amp; b \leq -\lambda \\
0, &amp; |b| &lt; \lambda \\
b - \lambda, &amp; b \geq \lambda
\end{cases}
\tag{7.2}\]</span></span></p>
<p>Through this approach, one can solve the Lasso problem without explicitly involving the matrix X, significantly simplifying the computational process, especially in high-dimensional settings where matrix operations become impractical. This adaptation aligns with the objectives of proximal gradient methods which aim to efficiently handle large-scale optimization problems.</p>
</section>
<section id="detailed-derivation-of-proximal-gradient-descent-for-lasso" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="detailed-derivation-of-proximal-gradient-descent-for-lasso"><span class="header-section-number">7.2</span> Detailed Derivation of Proximal Gradient Descent for Lasso</h2>
<p>To utilize Proximal Gradient Descent as per the soft thresholding equation <a href="#eq-pgd-softt" class="quarto-xref">Equation&nbsp;<span>7.2</span></a>, the Lasso problem must be reformulated into a form that does not involve the matrix X.</p>
<p>First, for convex function <span class="math inline">\(g(x)\)</span>, according to Lipschitz continuity, for <span class="math inline">\(\forall\)</span> x, y, there always exists a constant L s.t.</p>
<p><span class="math display">\[
\left| f'(y) - f'(x) \right| \leq t \|y - x\|_2.
\]</span></p>
<p>Then using a second-order Taylor series，g(x) can be expressed as:</p>
<p><span class="math display">\[
\begin{align}
g(x) &amp;= g(x_0) + \nabla g(x_0)^T (x - x_0) + \frac{1}{2} \nabla^2 g(x_0) (x - x_0)^2 \\
&amp;= g(x_0) + \nabla g(x_0)^T (x - x_0) + \frac{1}{2t} (x - x_0)^2
\end{align}
\]</span></p>
<p>Hence, lasso problem now is</p>
<p><span id="eq-pgd-lasso"><span class="math display">\[
\arg \min_{x} \left[ g(x_0) + \nabla g(x_0)^T (x - x_0) + \frac{1}{2t} \|x - x_0\|^2_2 + \lambda \|x\|_1 \right]
\tag{7.3}\]</span></span></p>
<p>Since <span class="math inline">\(g(x_0)\)</span> is constant, <a href="#eq-pgd-lasso" class="quarto-xref">Equation&nbsp;<span>7.3</span></a> is equivalent to</p>
<p><span class="math display">\[
\arg \min_{x} \left[ \nabla g(x_0)^T (x - x_0) + \frac{1}{2t} \|x - x_0\|^2_2 + \lambda \|x\|_1 \right]
\]</span></p>
<p>To balance the terms, we introduce a constant <span class="math inline">\((t\nabla g(x_0))^2\)</span> .By doing this, the Lasso problem then becomes</p>
<p><span class="math display">\[
\arg \min_{x}  \frac{1}{2t} \|x - x_0 - t\nabla g(x_0)\|^2_2 + \lambda \|x\|_1
\]</span></p>
<p>Denote <span class="math inline">\(z = x_0 - t\nabla g(x_0)\)</span></p>
<p>Thus,</p>
<p><span class="math display">\[
x^{*} = \arg \min_{x} \left( \|x - z\|_2^2 + t \lambda \|x\|_1 \right)
\]</span></p>
<p>Recall the soft thresholding function <span class="math inline">\(S_{\lambda}(b)\)</span> denoted as <a href="#eq-pgd-softt" class="quarto-xref">Equation&nbsp;<span>7.2</span></a> , which is used in the context of the Lasso problem, reformulated in the form of <a href="#eq-pgd-lassonoA" class="quarto-xref">Equation&nbsp;<span>7.1</span></a> . The optimal solution <span class="math inline">\(x^*\)</span> can be determined by applying <span class="math inline">\(S_{\lambda}(z)\)</span>, which is known as the proximal operator.</p>
<p><span class="math display">\[
\text{prox}_{t,h()}(z)=S_{\lambda}(z)=
\begin{cases}
z + \lambda, &amp; z \leq -\lambda \\
0, &amp; |z| &lt; \lambda \\
z - \lambda, &amp; z \geq \lambda
\end{cases}
\]</span></p>
</section>
<section id="proximal-gradient-algorithm" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="proximal-gradient-algorithm"><span class="header-section-number">7.3</span> Proximal Gradient Algorithm</h2>
<ol type="1">
<li>Set k=0 and intial point <span class="math inline">\(x^0\)</span></li>
</ol>
<p>Repeat</p>
<ol start="2" type="1">
<li><p>Compute z by <span class="math inline">\(z^{(k)} = x^{(k)} - t\nabla g(x^{(k)})\)</span></p></li>
<li><p>Compute proximal operator by <span class="math inline">\(\text{prox}_{t,h()}(z^{(k)}) = S_{\lambda}(z^{(k)})\)</span></p></li>
<li><p>Set <span class="math inline">\(x^{(k+1)}\)</span> by <span class="math inline">\(x^{(k+1)}=\text{prox}_{t,h()}(z^{(k)})\)</span></p></li>
</ol>
<p>until convergence or max number of iterations;</p>
</section>
</section>
<section id="evaluation-of-performance-of-algorithms-for-lasso" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Evaluation of performance of Algorithms for Lasso</h1>
<p>When comparing three algorithms for solving the Lasso regression problem, we primarily focus on two dimensions of performance: convergence speed and computational efficiency. The convergence curves illustrate the trend of the objective function value with the number of iterations, while the computation time reflects the running efficiency of each algorithm.</p>
<p>To test performance of algorithms, a synthetic Gaussian data with N observations and p predictors which is used for test in J.Friedman’s work <span class="citation" data-cites="glmnet-lasso"><a href="proposal.html#ref-glmnet-lasso" role="doc-biblioref">[2]</a></span>.</p>
<p>Following is the results of test.</p>
<p><img src="images/paste-14.png" class="img-fluid"></p>
<table class="table">
<caption>Time Consumption of Lasso Algorithms Using Synthetic Data</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Subgradient Descent</th>
<th>Coordinate Descent</th>
<th>Proximal Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time Spent</td>
<td>0.5877</td>
<td>0.0090</td>
<td>0.0823</td>
</tr>
</tbody>
</table>
<p>Observing the convergence curves, Coordinate Descent demonstrates a notably quick convergence characteristic, achieving lower objective function values with fewer iterations compared to the other two algorithms. The Subgradient Descent method converges very slowly, and it is noticeable that the f(x) does not consistently decrease, which aligns with our theoretical understanding that Subgradient Descent does not guarantee monotonicity. The Proximal Gradient Descent method has a medium speed of convergence among the three, but its initial phase of convergence is quite significant.</p>
<p>In terms of computation time, Coordinate Descent significantly outperforms the other methods, exhibiting the lowest time consumption, highlighting the efficiency of Coordinate Descent. The time spent by Proximal Gradient Descent is higher but still substantially less than that of Subgradient Descent.</p>
<p>Taking into account both the convergence curves and time spent, Coordinate Descent provides the best balance of performance for handling the Lasso problem with synthetic data. It is not only fast in terms of convergence but also high in operational efficiency. This can be attributed to its approach of updating one variable at a time, which reduces the computational load and potentially leverages the structural properties of the problem.</p>
</section>
<section id="conclusion" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Conclusion</h1>
<p>The LASSO problem involves a non-differentiable function, which cannot be handled by the standard gradient descent method. This has led to the adoption of the subgradient descent method as a solution. The Subgradient method provides a straightforward approach; however, it does not guarantee descent in each iteration. That is slow and does not yield a sparse model. Alternative methods based on the concept of subgradients, such as Coordinate Descent and the Proximal Gradient method, are employed. Coordinate Descent excels in high-dimensional spaces, making it ideal for sparse models, and its computational speed is the reason for its widespread industrial use in implementing the Lasso algorithm.</p>
</section>
<section id="reference" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Reference</h1>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-textbook" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">S. textbook-Boyd and L. Vandenberghe, <em>Convex optimization</em>. Cambridge University Press, 2004. Available: <a href="https://web.stanford.edu/~boyd/cvxbook/">https://web.stanford.edu/~boyd/cvxbook/</a></div>
</div>
<div id="ref-glmnet-lasso" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">J. Friedman, T. Hastie, and R. Tibshirani, <span>“<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880">Regularization paths for generalized linear models via coordinate descent</a>,”</span> <em>Journal of Statistical Software</em>, vol. 33, no. 1, pp. 1–22, 2010.</div>
</div>
<div id="ref-ESL" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">T. Hastie, R. Tibshirani, and J. Friedman, <em>The elements of statistical learning: Data mining, inference, and prediction</em>, 2nd ed. Springer, 2009. Available: <a href="https://hastie.su.domains/ElemStatLearn/">https://hastie.su.domains/ElemStatLearn/</a></div>
</div>
<div id="ref-question1" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline"><span>“Why doesn’t subgradient descent give sparse solutions to the lasso?”</span> Available: <a href="https://www.quora.com/Why-doesnt-subgradient-descent-give-sparse-solutions-to-the-Lasso">https://www.quora.com/Why-doesnt-subgradient-descent-give-sparse-solutions-to-the-Lasso</a></div>
</div>
<div id="ref-question2" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline"><span>“Why proximal gradient descent instead of plain subgradient methods for lasso?”</span> Available: <a href="https://stats.stackexchange.com/questions/177800/why-proximal-gradient-descent-instead-of-plain-subgradient-methods-for-lasso/226050#226050">https://stats.stackexchange.com/questions/177800/why-proximal-gradient-descent-instead-of-plain-subgradient-methods-for-lasso/226050#226050</a></div>
</div>
<div id="ref-FISTA" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">A. Beck and M. Teboulle, <span>“A fast iterative shrinkage-thresholding algorithm for linear inverse problems,”</span> <em>SIAM Journal on Imaging Sciences</em>, vol. 2, no. 1, pp. 183–202, 2009, doi: <a href="https://doi.org/10.1137/080716542">10.1137/080716542</a>. Available:<a href=" https://doi.org/10.1137/080716542"> https://doi.org/10.1137/080716542</a></div>
</div>
</div>
</section>
<section id="appendix" class="level1 unnumbered">
<h1 class="unnumbered">Appendix</h1>
<p>https://github.com/Siyu-789/CONVEXOPT_Proj</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./proposal.html" class="pagination-link" aria-label="LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./project_code.html" class="pagination-link" aria-label="project_code.html">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">project_code.html</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>