[
  {
    "objectID": "project_code.html",
    "href": "project_code.html",
    "title": "3Â  Code of Project",
    "section": "",
    "text": "import numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\nimport time\n\n\nclass LassowithCG:\n    \"\"\"\n    The optimization problem of LASSO is \n        1/(2*n) * ||y-Xw||^2_2 + alpha * ||w||_1\n    \"\"\"\n    def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n                copy_X=True, max_iter=1000, tol=1e-5, selection='cyclic'):\n        self._alpha = alpha\n        self._fit_intercept = fit_intercept\n        self._normalize = normalize\n        self._copy_X = copy_X\n        self._max_iter = max_iter\n        self._tol = tol\n        self._selection = selection\n        \n        if normalize:\n            self._scaler = preprocessing.StandardScaler()\n            \n    def compute_step(self, k, X, y, coef, intercept, alpha):\n        n, p = X.shape\n        y_predict = np.dot(X, coef) + intercept\n        pk = np.dot(X[:,k], y-y_predict + X[:,k] * coef[k])\n        mk = np.linalg.norm(X[:,k], ord=2) ** 2\n        coef_k = np.max([pk-alpha, 0]) - np.max([-pk-alpha, 0])\n        coef_k = coef_k / (1.0 * mk)\n        \n        return coef_k\n    \n    def objective(self, X, y, coef, intercept, alpha):\n        n, p = X.shape\n        total = 0\n        \n        y_predict = np.dot(X, coef) + intercept\n        total += \\\n            1/(2.0) * np.linalg.norm(y-y_predict, ord=2) ** 2\n        total += alpha * np.linalg.norm(coef, ord=1)\n        \n        return total\n        \n    def fit(self, X, y):\n        if self._copy_X:\n            X = X.copy()\n        if self._normalize:\n            X = self._scaler.fit_transform(X)\n        self._objectives = []\n        \n        # initialize data\n        num_samples, num_features = X.shape\n        coef = np.zeros(num_features)\n        old_coef = np.zeros(num_features)\n        intercept = 0\n        if self._fit_intercept:\n            tmp = y - np.dot(X, coef)\n            intercept = np.sum(tmp) / (1.0 * num_samples)\n        num_iters = 0\n        for iter in range(self._max_iter):\n            num_iters = num_iters + 1\n            if (self._selection == \"cyclic\"):\n                for k in range(num_features):\n                    old_coef[k] = coef[k]\n                    coef[k] = self.compute_step(k, X, y, coef, intercept, self._alpha)\n                if self._fit_intercept:\n                    tmp = y - np.dot(X, coef)\n                    intercept = np.sum(tmp) / (1.0 * num_samples)\n                # check conditions of convergence\n                coef_updates = np.abs(coef - old_coef)\n                if np.max(coef_updates) &lt; self._tol:\n                    break\n            self._objectives.append(self.objective(X, y, coef, intercept, self._alpha))\n        \n        self._coef = coef\n        self._intercept = intercept\n        self._num_iters = num_iters\n        \n        return self\n        \n    def predict(self, X):\n        if self._copy_X:\n            X = X.copy()\n        if self._normalize:\n            X = self._scaler.transform(X)\n        \n        y_predict = np.dot(X, self._coef) + self._intercept\n        \n        return y_predict\n        \n    def score(self, X, y):\n        y_predict = self.predict(X)\n    \n        return r2_score(y, y_predict)\n    \n    @property\n    def coef_(self):\n        return self._coef\n    \n    @property\n    def intercept_(self):\n        return self._intercept\n        \n    @property\n    def n_iter_(self):\n        return self._num_iters\n        \n    @property\n    def objectives_(self):\n        return self._objectives\n    \n    \n    def __str__(self):\n        return (\"Lasso(alpha={}, copy_X={}, \"\n                \"fit_intercept={}, max_iter={}, \"\n                \"normalize={}, selection=\\'{}\\', \"\n                \"tol={})\").format(self._alpha, self._copy_X,\n                                  self._fit_intercept, self._max_iter,\n                                  self._normalize, self._selection,\n                                  self._tol)\n\n\n# Define the number of observations N and predictors p\nN, p = 100, 10  # Example values\nrho = 0.95  # Example value for population correlation\n\n# Create correlation matrix\ncorr_matrix = rho * np.ones((p, p)) + (1 - rho) * np.eye(p)\n\n# Generate Gaussian data using the correlation matrix\nmean = np.zeros(p)\nX = np.random.multivariate_normal(mean, corr_matrix, size=N)\n\n# Define the coefficients\nbeta = np.array([(-1)**j * np.exp(-2*j / 20) for j in range(p)])\n\n# Generate Gaussian noise\nZ = np.random.normal(0, 1, N)\n\n# Compute k to achieve the desired SNR of 3.0\nsignal_power = np.var(np.dot(X, beta))\nnoise_power = signal_power / 3.0**2  # SNR = signal_power / noise_power\nk = np.sqrt(noise_power)\n\n# Generate the outcome values Y\nY = np.dot(X, beta) + k * Z\n\n\n# Initialize and fit the Lasso model\nlasso_CG = LassowithCG(alpha=0.1, max_iter=1000, tol=1e-4)\n\nt0=time.time()\nlasso_CG.fit(X, Y)\nrun_time_CG=time.time()-t0\n\n# Plot the convergence curve\nplt.figure(figsize=(10, 6))\nplt.plot(lasso_CG.objectives_, label='Lasso Coordinate Descent')\nplt.xlabel('k')\nplt.ylabel('f(x)')\nplt.title('Lasso Convergence Curve with Synthetic Data')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nlasso_CG.objectives_\n\n[10.215051584122934,\n 9.056300654514542,\n 8.087185363302886,\n 7.263717204966501,\n 6.563366415332416,\n 5.96722957399569,\n 5.459356996725757,\n 5.026284958826633,\n 4.656643245636209,\n 4.34082525404853,\n 4.070710600714908,\n 3.8394318171247495,\n 3.6411780689956847,\n 3.4710299751805636,\n 3.3248205551666348,\n 3.199018133962631,\n 3.090627703691949,\n 2.997107803447369,\n 2.916300450443008,\n 2.846372050912062,\n 2.785763550855896,\n 2.7331483649674304,\n 2.687396855475036,\n 2.647546328526304,\n 2.612775680124617,\n 2.582383961631811,\n 2.5557722507196825,\n 2.532428310963387,\n 2.5119136050138264,\n 2.4938522949722404,\n 2.477921921320682,\n 2.463845500298675,\n 2.451384820436222,\n 2.440334753294642,\n 2.4305184223675123,\n 2.421783098425123,\n 2.41399671007715,\n 2.407044875592636,\n 2.4008283765674303,\n 2.3952610062986035,\n 2.3902677360748403,\n 2.3857831513256684,\n 2.3817501169458155,\n 2.3781186373386,\n 2.3748448819843646,\n 2.3718903517881453,\n 2.3692211652225383,\n 2.36680744646433,\n 2.3646228004172327,\n 2.3626438617940004,\n 2.3608499073633915,\n 2.359222522104772,\n 2.357745311401321,\n 2.356403652579954,\n 2.3551844801048567,\n 2.354076099579123,\n 2.3530680264286055,\n 2.352150845753222,\n 2.3513160903501515,\n 2.35055613435454,\n 2.349864100318429,\n 2.34923377786747,\n 2.3486595523462865,\n 2.34813634209413,\n 2.34765954318887,\n 2.347224980664616,\n 2.346828865350645,\n 2.346467755600643,\n 2.3461385232846403,\n 2.3458383235042097,\n 2.345564567566614,\n 2.345314898817813,\n 2.345087170988969,\n 2.3448794287579373,\n 2.3446898902671895,\n 2.3445169313739154,\n 2.3443590714372964,\n 2.344214960473147,\n 2.344083367527627,\n 2.3439631701403094,\n 2.343853344782789,\n 2.3437529581727903,\n 2.3436611593755785,\n 2.3435771726147325,\n 2.343500290723216,\n 2.3434298691734368,\n 2.343365320631622,\n 2.343306109987795,\n 2.343251749817659,\n 2.343201796237272,\n 2.3431558451152763,\n 2.34311352861099,\n 2.3430745120096836,\n 2.3430384908291706,\n 2.343005188174181,\n 2.342974352317234,\n 2.342945754486631,\n 2.3429191868438806,\n 2.342894460634535,\n 2.3428714044977106,\n 2.3428498629209207,\n 2.3428296948279375,\n 2.3428107722885136,\n 2.342792979339654,\n 2.3427762109090815,\n 2.342760371832249,\n 2.342745375955039,\n 2.342731145314888,\n 2.3427176093936994,\n 2.34270470443649,\n 2.342692372830131,\n 2.342680562537128,\n 2.342669226579687,\n 2.3426583225697866,\n 2.3426478122812995,\n 2.342637661260542,\n 2.3426278384719157,\n 2.342618315975614,\n 2.342609068634586,\n 2.3426000738482085,\n 2.342591311310324,\n 2.3425827627894815,\n 2.3425744119294345,\n 2.342566244068083,\n 2.3425582460732226,\n 2.342550406193581,\n 2.3425427139237778,\n 2.3425351598819386,\n 2.342527735698816,\n 2.342520433917366,\n 2.3425132479018203,\n 2.3425061717553786,\n 2.3424992002457143,\n 2.3424923287375834,\n 2.342485553131852,\n 2.342478869810353,\n 2.3424722755860126,\n 2.3424657676577496,\n 2.342459343569701,\n 2.342453001174345,\n 2.34244673859916,\n 2.3424405542164752,\n 2.3424344466161973,\n 2.3424284145811547,\n 2.342422457064788,\n 2.342416573170954,\n 2.3424107621356773,\n 2.3424050233106026,\n 2.3423993561480345,\n 2.342393760187384,\n 2.3423882350428973,\n 2.3423827803925406,\n 2.3423773959679317,\n 2.3423720815452262,\n 2.342366836936859,\n 2.3423616619840715,\n 2.3423565565501483,\n 2.342351520514306,\n 2.3423465537661627,\n 2.342341656200762,\n 2.3423368277140773,\n 2.342332068198979,\n 2.3423273775416105,\n 2.342322755618155,\n 2.3423182022919518,\n 2.3423137174109376,\n 2.3423093008053955,\n 2.342304952285982,\n 2.3423006716420156,\n 2.342296458640005,\n 2.3422923130224103,\n 2.3422882345066105,\n 2.3422842227840626,\n 2.3422802775196576,\n 2.34227639835123,\n 2.342272584889253,\n 2.342268836716649,\n 2.3422651533887735,\n 2.342261534433503,\n 2.342257979351456,\n 2.342254487616321,\n 2.342251058675297,\n 2.342247691949608,\n 2.342244386835133,\n 2.34224114270309,\n 2.342237958900816,\n 2.342234834752593,\n 2.3422317695605464,\n 2.342228762605594,\n 2.3422258131484366,\n 2.3422229204305967,\n 2.342220083675486,\n 2.3422173020895074,\n 2.3422145748631795]\n\n\n\nclass SubGradient(object):\n    def __init__(self, A, b, mu, iteration=1000, tol=1e-9):\n        self.x = None\n        self.A = A\n        self.m, self.n = self.A.shape\n        self.q = np.dot(A.T, A)\n        self.step_size = 1.0 / np.linalg.norm(self.q, 2)\n\n        self.b = b\n        self.Atb = np.dot(A.T, self.b)\n        self.mu = mu\n\n        self.iteration = iteration\n        self.obj_path = [1]\n        self.tol = tol\n        self.initers = 0\n        self.iters = 0\n        self.run_time = 0\n\n    def loss(self, x):\n        x = x.reshape(-1)\n        return 0.5 * np.sum(np.square(np.dot(self.A, x) - self.b)) + self.mu * np.sum(np.abs(x))\n\n    def fix_step(self, mu, x):\n        g = np.dot(self.q, x) - self.Atb + np.sign(x) * mu\n        x -= self.step_size * g\n        return x\n\n    def dimish_step(self, mu, x, iter):\n        g = np.dot(self.q, x) - self.Atb + np.sign(x) * mu\n        x -= self.step_size / iter * g\n        return x\n\n    def lenth_step(self, mu, x):\n        g = np.dot(self.q, x) - self.Atb + np.sign(x) * mu\n        x -= self.step_size / np.linalg.norm(g) * g\n        return x\n\n    def train(self):\n        t0 = time.time()\n        x = np.zeros(self.n)\n        print(\"subgradient method begins\")\n        self.initers = 0\n        for hot_mu in [1e3, 1e2, 1e1, 1e-1, 1e-2, 1e-3]:\n            err_rate = 1.0\n            in_iter = 1\n            while err_rate &gt; self.tol and in_iter &lt; self.iteration:\n                x = self.fix_step(hot_mu, x)\n                self.obj_path.append(self.loss(x))\n                err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                in_iter += 1\n            self.initers += in_iter\n\n        self.iters = 1\n        err_rate = 1.0\n        while err_rate &gt; self.tol:\n            x = self.dimish_step(self.mu, x, self.iters)\n            self.obj_path.append(self.loss(x))\n            err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n            self.iters += 1\n\n        self.x = x\n        self.run_time = time.time() - t0\n        print(\"subgradient obj: {: &gt;4.9f}/ time: {: &gt;4.4f} /initers: {}/ iters: {}\".format(self.obj_path[-1],\n                                                                                           self.run_time,\n                                                                                           self.initers,\n                                                                                           self.iters))\n\n\nsubgrad = SubGradient(X, Y, mu=0.1, iteration=1000)\n\nt0=time.time()\nsubgrad.train()\nrun_time_subgrad=time.time()-t0\n\n# plot\nplt.figure(figsize=(10, 6))\nplt.plot(subgrad.obj_path[2:], label='SubGradient Objective') \nplt.xlabel('k')\nplt.ylabel('f(x)')\nplt.title('Lasso Convergence Curve with Synthetic Data')\nplt.legend()\nplt.show()\n\nsubgradient method begins\nsubgradient obj: 2.353513947/ time: 0.3864 /initers: 6000/ iters: 30535\n\n\n\n\n\n\n\n\n\n\nsubgrad.obj_path[2:10]\n\n[4111.848088446432,\n 4112.020097245703,\n 4111.652941886048,\n 4111.824835811962,\n 4111.461536518525,\n 4111.633298338927,\n 4111.273796651493,\n 4111.4454097053285]\n\n\n\nclass LassowithProxGradient:\n    def __init__(self,  A, b, mu, init_iteration=100, max_iteration=2000, tol=1e-8):\n        self.x = None\n        self.A = A\n        self.m, self.n = self.A.shape\n        self.q = np.dot(A.T, A)\n        self.step_size = 1.0 / np.linalg.norm(self.q, 2)\n\n        self.b = b\n        self.Atb = np.dot(self.A.T, self.b)\n        self.mu = mu\n\n        self.init_iteration = init_iteration\n        self.max_iteration = max_iteration\n        self.obj_path = [1]\n        self.tol = tol\n        self.iters = 1\n        self.run_time = 0\n\n    def loss(self, x):\n        x = x.reshape(-1)\n        return 0.5 * np.sum(np.square(np.dot(self.A, x) - self.b)) + self.mu * np.sum(np.abs(x))\n\n    @staticmethod\n    def prox(x, num):\n        def pointwise_prox(u, t):\n            if u &gt;= t:\n                return u - t\n            elif u &lt;= -t:\n                return u + t\n            else:\n                return 0.0\n        return np.vectorize(pointwise_prox)(x, num)\n\n    def basic_step(self, mu, x):\n        g = np.dot(self.q, x) - self.Atb\n        x = x - self.step_size * g\n        x = self.prox(x, mu * self.step_size)\n        return x\n\n    def fast_step(self, mu, x, x_, k):\n        # we could reformulate FISTA in a nesterov-like form involving x and v\n        y = x + 1.0 * (k - 2)/(k + 1) * (x - x_)\n        x_ = x.copy()\n        g = np.dot(self.q, y) - self.Atb\n        x = y - self.step_size * g\n        x = self.prox(x, mu * self.step_size)\n        return x, x_\n\n    def nesterov_step(self, mu, x, v, k):\n        theta = 2.0 / (k + 1)\n        y = (1.0 - theta) * x + theta * v\n        g = np.dot(self.q, y) - self.Atb\n        tmp = v - self.step_size / theta * g\n        v = self.prox(tmp, mu * self.step_size / theta)\n        x = (1.0 - theta) * x + theta * v\n        return x, v\n\n    def train(self, mode=\"Basic\"):\n        t0 = time.time()\n\n        x = np.random.normal(size=self.n)\n        print(\"{} proximal gradient begins\".format(mode))\n        self.initers = 0\n        hot_mus = [1e3, 1e2, 1e1, 1e-1, 1e-2, 1e-3]\n        if mode == 'Basic':\n            for i in range(len(hot_mus)):\n                err_rate = 1.0\n                in_iter = 1\n                while err_rate &gt; 10**(-5-i) and in_iter &lt; self.max_iteration:\n                    x = self.basic_step(hot_mus[i], x)\n                    self.obj_path.append(self.loss(x))\n                    err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                    in_iter += 1\n                self.initers += in_iter\n\n            self.iters = 1\n            err_rate = 1.0\n            while err_rate &gt; self.tol and self.initers &lt; self.max_iteration:\n                x = self.basic_step(self.mu, x)\n                self.obj_path.append(self.loss(x))\n                err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                print(err_rate)\n                self.iters += 1\n\n        elif mode == 'FISTA':\n            x_ = x.copy()\n            for i in range(len(hot_mus)):\n                err_rate = 1.0\n                in_iter = 1\n                while err_rate &gt; 10**(-5-i) and in_iter &lt; self.init_iteration:\n                    x, x_ = self.fast_step(hot_mus[i], x, x_, in_iter)\n                    self.obj_path.append(self.loss(x))\n                    err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                    in_iter += 1\n                self.initers += in_iter\n\n            self.iters = 1\n            err_rate = 1.0\n            while err_rate &gt; self.tol and self.iters &lt; self.max_iteration:\n                x, x_ = self.fast_step(self.mu, x, x_, self.iters)\n                self.obj_path.append(self.loss(x))\n                err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                self.iters += 1\n\n        elif mode == 'Nesterov':\n            v = x.copy()\n            for i in range(len(hot_mus)):\n                err_rate = 1.0\n                in_iter = 1\n                while err_rate &gt; 10**(-5-i) and in_iter &lt; self.init_iteration:\n                    x, v = self.nesterov_step(hot_mus[i], x, v, in_iter)\n                    self.obj_path.append(self.loss(x))\n                    err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                    in_iter += 1\n                self.initers += in_iter\n\n            self.iters = 1\n            err_rate = 1.0\n            while err_rate &gt; self.tol and self.max_iteration:\n                x, v = self.nesterov_step(self.mu, x, v, self.iters)\n                self.obj_path.append(self.loss(x))\n                err_rate = np.abs(self.obj_path[-1] - self.obj_path[-2]) / self.obj_path[-2]\n                self.iters += 1\n\n        else:\n            raise ValueError(\"No {} mode. Choose modes from Basic(Default), FISTA or Nesterov\")\n\n        self.x = x\n        self.run_time = time.time() - t0\n        print(\"{:s} proximal gradient obj: {: &gt;4.9f}/ time: {: &gt;4.4f} /initers: {}/ iters: {}\".format(mode,\n                                                                                                      self.obj_path[-1],\n                                                                                                      self.run_time,\n                                                                                                      self.initers,\n                                                                                                      self.iters))\n\n\nlasso_pgd = LassowithProxGradient(X, Y, mu=0.1, init_iteration=10, max_iteration=1000, tol=0.001)\n\nt0=time.time()\nlasso_pgd.train()\nrun_time_pgd=time.time()-t0\n\n# plot\nplt.figure(figsize=(10, 6))\nplt.plot(lasso_pgd.obj_path[1:], label='Proximate Gradient Descent') \nplt.xlabel('k')\nplt.ylabel('f(x)')\nplt.title('Lasso Convergence Curve with Synthetic Data')\nplt.legend()\nplt.show()\n\nBasic proximal gradient begins\nBasic proximal gradient obj: 2.358819703/ time: 0.1060 /initers: 3837/ iters: 1\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18, 6)) \n\naxs[0].plot(subgrad.obj_path[2:], label='Subgradient Descent')\naxs[0].set_xlabel('k')\naxs[0].set_ylabel('f(x)')\naxs[0].set_title('Lasso Convergence Curve with mu=0.1')\naxs[0].legend()\n\naxs[1].plot(lasso_CG._objectives, label='Coordinate Descent')\naxs[1].set_xlabel('k')\naxs[1].set_ylabel('f(x)')\naxs[1].set_title('Lasso Convergence Curve with mu=0.1')\naxs[1].legend()\n\naxs[2].plot(lasso_pgd.obj_path[1:], label='Proximate Gradient Descent')\naxs[2].set_xlabel('k')\naxs[2].set_ylabel('f(x)')\naxs[2].set_title('Lasso Convergence Curve with mu=0.1')\naxs[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nlist=(run_time_subgrad,run_time_CG,run_time_pgd)\nlist\n\n(0.506633996963501, 0.010000467300415039, 0.10702157020568848)",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Code of Project</span>"
    ]
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "2Â  Report",
    "section": "",
    "text": "3 Introduction\nThe Least Absolute Shrinkage and Selection Operator (Lasso) method is a cornerstone of regression analysis, known for its ability to both select variables and regularize data to enhance model interpretability and prediction accuracy. Lasso optimization involves solving a convex optimization problem that is not necessarily differentiable due to the absolute value in its regularization term. This term project explores three distinct optimization algorithms that address these challenges: the Subgradient method, Coordinate Descent, and the Proximal Gradient method. Through a comparative analysis, this report aims to highlight the practical performances, strengths, and weaknesses of each algorithm within the context of Lasso optimization.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#section",
    "href": "Project.html#section",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "7.2 ",
    "text": "7.2",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#background-of-lasso",
    "href": "Project.html#background-of-lasso",
    "title": "2Â  Report",
    "section": "4.1 Background of Lasso",
    "text": "4.1 Background of Lasso\nIn 1996, Robert Tibshirani first introduced the Lasso, a method now commonly encountered in various fields. The Lasso technique is designed to enhance the prediction accuracy and interpretability of the statistical models by altering the regularization process.\nThe form of Lasso is given by the following optimization problem:\n\\[\n\\begin{align}\n\\min_{\\beta} \\left\\{ \\left\\| y - X\\beta \\right\\|_2^2 \\right\\} \\\\\n\\text{subject to} \\quad \\left\\| \\beta \\right\\|_1 \\leq s\n\\end{align}\n\\tag{4.1}\\]\nHere, \\(y \\in \\mathbb{R}^n\\) represents the response vector, \\(X \\in \\mathbb{R}^{n*p}\\) is the matrix of predictors, \\(\\beta \\in \\mathbb{R}^{p}\\) denotes the coefficient vector and \\(s\\in \\mathbb{R}_+\\) determining the degree of regularization\nSee FigureÂ 4.1 illustrates the simple situation of p=2.\n\n\n\n\n\n\nFigureÂ 4.1: Contours of the error and constraint functions for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regions, \\(|Î²_1| + |Î²_2| â¤ s\\) and \\(Î²_1^2 + Î²_2^2 â¤ s\\), while the red ellipses are the contours ofthe RSS.\n\n\n\nEquationÂ 4.1 is equivalent to the following unconstrained optimization problem by logrange dual function.\n\\[\n\\begin{equation}\n\\min_{\\beta} \\left\\{\\left\\| y - X\\beta \\right\\|_2^2 + \\lambda \\left\\| \\beta \\right\\|_1 \\right\\}\n\\end{equation}\n\\tag{4.2}\\]\nHere, \\(\\lambda \\in R^+\\). The choice of ð controls the trade-off between the sparsity of the model ð½ and the fit of the model.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#lasso-cannot-use",
    "href": "Project.html#lasso-cannot-use",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "4.2 Lasso cannot use",
    "text": "4.2 Lasso cannot use\nIn unconstrained optimization problems as Boyd Stephen said see [1, Ch. 9]:",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "1Â  Proposal",
    "section": "",
    "text": "2 Introduction\nThe Least Absolute Shrinkage and Selection Operator (LASSO) is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. The optimization of LASSO involves solving a convex but not necessarily differentiable problem due to the absolute value in the regularization term. This term project aims to explore, implement, and compare three distinct optimization algorithms not covered in our standard curriculum: the Subgradient method, Coordinate Descent, and the Proximal Gradient method, each of which offers a unique approach to tackling the challenges posed by the LASSO formulation.\n\n\n3 Rationale for the Project\nThe motivation behind selecting these three methods lies in their relevance and varied approaches to handling convex optimization problems, especially in dealing with the L1 norm regularization. The Subgradient method is one of the most straightforward approaches to deal with non-differentiability, providing a robust but potentially slower convergence to the solution. Coordinate Descent is highly efficient in high-dimensional spaces, making it particularly suitable for sparse models like LASSO. Lastly, the Proximal Gradient method is an elegant compromise between efficiency and speed, especially with its accelerated variants like FISTA, which can significantly speed up convergence.\nBy comparing these methods, this project aims to shed light on their practical performances, advantages, and limitations within the specific context of LASSO optimization. This comparison will not only enrich our understanding of these algorithms but also provide insights into their suitability for different types of LASSO problems, potentially guiding future applications and research.\n\n\n4 Scope of the project\n\nLASSO function\nLearning notes of methods:\n\n\nDefinition\nProcedure of algorithm and iterative formula\nConvergence analysis\n\n\nAlgorithm implementation\nComparative analysis:\n\n\nTiming: The run times of various algorithms are to be compared using datasets generated in accordance with the approach as delineated by Friedman, Hastie, and Tibshirani (2010). These datasets will simulate realistic correlation structures and signal-to-noise ratios to ensure the timing comparisons are representative of practical scenarios.\nConvergence Speed Analysis\n\n\nDiscussion and Trade-off of Methods\n\n\n\n5 Reference\n\n\nBeck, Amir, and Marc Teboulle. 2009. âA Fast Iterative\nShrinkage-Thresholding Algorithm for Linear Inverse Problems.â\nSIAM Journal on Imaging Sciences 2 (1): 183â202. https://doi.org/10.1137/080716542.\n\n\nFriedman, J., T. Hastie, and R. Tibshirani. 2010. âRegularization\nPaths for Generalized Linear Models via Coordinate Descent.â\nJournal of Statistical Software 33 (1): 1â22.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\ntextbook-Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex\nOptimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/.\n\n\nâWhy Doesnât Subgradient Descent Give Sparse Solutions to the\nLasso?â n.d. https://www.quora.com/Why-doesnt-subgradient-descent-give-sparse-solutions-to-the-Lasso.\n\n\nâWhy Proximal Gradient Descent Instead of Plain Subgradient\nMethods for Lasso?â n.d. https://stats.stackexchange.com/questions/177800/why-proximal-gradient-descent-instead-of-plain-subgradient-methods-for-lasso/226050#226050.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "Project.html#challenges-of-gradient-descent-method-on-lasso-problem",
    "href": "Project.html#challenges-of-gradient-descent-method-on-lasso-problem",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "4.2 Challenges of Gradient Descent Method on Lasso Problem",
    "text": "4.2 Challenges of Gradient Descent Method on Lasso Problem\nIn unconstrained optimization problems as Boyd Stephen said see [1, Ch. 9]:\n\\[\nmin_{x\\in C}f(x)\n\\tag{4.3}\\]\nwhere $f(x) is a convex function, and \\(C=domf\\) represents a convex set. When \\(f(x)\\) is differentiable, the optimality condition is defined as: \\[\nmin_{x\\in C}f(x)\n\\tag{4.4}\\]\nHowever, LASSO is non-differentiable. To solve this problem, the concept of a subgradient will be introduced later in this report, serving as a generalization of the gradient.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#convexity-and-differentiability-in-lasso-problem",
    "href": "Project.html#convexity-and-differentiability-in-lasso-problem",
    "title": "2Â  Report",
    "section": "4.2 Convexity and Differentiability in Lasso Problem",
    "text": "4.2 Convexity and Differentiability in Lasso Problem\nTo elucidate the optimization problem presented in EquationÂ 4.2, we denote the objective function by\n\\[\n\\begin{align}\nf(\\beta) &= g(\\beta) + h(\\beta), \\\\\n\\text{where } g(\\beta) &= \\left\\| y - X\\beta \\right\\|_2^2,\\\nh(\\beta) = \\lambda \\left\\| \\beta \\right\\|_1\n\\end{align}\n\\]\nThe Lasso optimization problem can then be rewritten from EquationÂ 4.2 as\n\\[\nmin_{\\beta \\in R^p}f(\\beta)\n\\]\nwhere the domain of the solution space is explicitly specified as \\(R^p\\).This reformulation aligns with the general optimization framework \\(min_{x\\in C}f(x)\\) and facilitates the subsequent analysis.\nIn the context of convexity, the objective function \\(f(\\beta)\\) is established as convex. This is attributed to it being the non-negative sum of \\(g(\\beta)\\), which itself is convex by virtue of being the composition of an affine function and the \\(L_2\\) norm, and \\(h(\\beta)\\), which is the \\(L_1\\) norm.\nWith regard to differentiability, the function \\(f(\\beta)\\) becomes non-differentiable because the term \\(h(\\beta)\\) is non-differentiable at the point \\(\\beta=0\\) despite the differentiability of \\(g(\\beta)\\).",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#challenges-of-gradient-descent-method-in-lasso-problem",
    "href": "Project.html#challenges-of-gradient-descent-method-in-lasso-problem",
    "title": "2Â  Report",
    "section": "4.3 Challenges of Gradient Descent Method in Lasso Problem",
    "text": "4.3 Challenges of Gradient Descent Method in Lasso Problem\nIn unconstrained optimization problems as Boyd Stephen said see [1, Ch. 9]:\n\\[\nmin_{x\\in C}f(x)\n\\tag{4.3}\\]\nwhere \\(f(x)\\) is a convex function, and \\(C=domf\\) represents a convex set. When \\(f(x)\\) is differentiable, the optimality condition is defined as: \\[\nf(x^*) = \\min_{x} f(x) \\Leftrightarrow \\nabla f(x^*)=0\n\\tag{4.4}\\]\nHowever, LASSO is non-differentiable. To solve this problem, the concept of a subgradient will be introduced later in this report, which extends the concept of a gradient to include non-differentiable functions.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#materials-of-subgradient",
    "href": "Project.html#materials-of-subgradient",
    "title": "2Â  Report",
    "section": "5.1 Materials of Subgradient",
    "text": "5.1 Materials of Subgradient\n\nTheorem 5.1 If a function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is both differentiable and convex, then for any two points \\(x, y\\) within a convex set \\(C \\subseteq \\mathbb{R}^n\\) , the following inequality holds:\n\\[\nf(y) \\geq f(x)+\\nabla f(x)^\\top (y - x)\n\\tag{5.1}\\]\n\nThe porperty of convexity shown in TheoremÂ 5.1 leads to the definition of subgradient as following:\n\nTheorem 5.2 For a function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), a vector \\(g \\in \\mathbb{R}^n\\) is called a subgradient of \\(f\\) at \\(x \\in dom f\\) if for all \\(x \\in \\text{dom} f\\),\n\\[\nf(x) \\geq f(x_0) + g^\\top (x - x_0)\n\\]\n\nIf f is convex and differentiable, then its gradient at x is a subgradient. However a subgradient can exist even when f is not differentiable at x. In such cases, the subdifferential at x consists of all vectors that satisfy the definition of a subgradient at the non-differentiable point. These vectors form a set which is a convex set that describes directions in which the value of the function ð decrease.\n\nTheorem 5.3 The subdifferential of a function \\(f\\) at a point \\(x_0\\) is defined as the set of subgradients of \\(f\\) at any \\(x\\in dom f\\):\n\\[\n\\partial f(x) = \\{ g : g \\text{ is a subgradient of } f \\text{ at } x \\in dom f\\}\n\\]\n\nThen the optimal condition is defined as: \\[\nf(x^*) = \\min_{x} f(x) \\Leftrightarrow 0 \\in \\partial f(x^*)\n\\tag{5.2}\\]",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#subgradient-optimization-algorithm",
    "href": "Project.html#subgradient-optimization-algorithm",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "5.2 Subgradient Optimization Algorithm",
    "text": "5.2 Subgradient Optimization Algorithm",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#convergence-analysis-of-algorithm",
    "href": "Project.html#convergence-analysis-of-algorithm",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "5.4 Convergence Analysis of Algorithm",
    "text": "5.4 Convergence Analysis of Algorithm",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#theoretical-convergence-rate-comparison",
    "href": "Project.html#theoretical-convergence-rate-comparison",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "8.1 Theoretical Convergence Rate Comparison",
    "text": "8.1 Theoretical Convergence Rate Comparison",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#empirical-performance-evaluation",
    "href": "Project.html#empirical-performance-evaluation",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "8.2 Empirical Performance Evaluation",
    "text": "8.2 Empirical Performance Evaluation",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#subgradient-on-lasso",
    "href": "Project.html#subgradient-on-lasso",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "5.2 Subgradient on Lasso",
    "text": "5.2 Subgradient on Lasso",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#subgradient-descent-algorithm",
    "href": "Project.html#subgradient-descent-algorithm",
    "title": "2Â  Report",
    "section": "5.3 Subgradient Descent Algorithm",
    "text": "5.3 Subgradient Descent Algorithm\nIn the iterative process, a single iteration step involves a forward movement from the current position along a certain direction. Specifically, during each iteration, based on the current position, a movement is made towards the negative gradient direction to find a new position, thus forming an iterative sequence. The iteration rule is:\n\\[\nx^k = x^{k-1} - t_k g^k, \\quad g^k \\in \\partial f(x^{k-1})\n\\tag{5.5}\\]\nDuring the iterative optimization process, each step is a finite improvement, which does not guarantee the overall convergence of the iterative sequence by itself. However, by selecting the best solution from all the iterations thus far, convergence towards an optimal solution encountered during the iteration process is achieved. The best solution obtained from all the iterations can be characterized as:\n\\[\nf(x^k_{\\text{best}}) = \\min \\{ f(x^i) \\}_{i=0,...,k}\n\\]\nThe complete subgradient descent algorithm process is as follows.\n\nSet k=0 and initial point \\(\\beta_1=\\beta_2=...=\\beta_p=0\\) and fixed step size t=1\n\nRepeat\n\nCompute the predicted values \\(\\hat Y=X\\beta\\)\nCalculate gradients by EquationÂ 5.3\nUpdata parameter by EquationÂ 5.5\n\nuntil termination test satisfied.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#subgradient-in-lasso",
    "href": "Project.html#subgradient-in-lasso",
    "title": "2Â  Report",
    "section": "5.2 Subgradient in Lasso",
    "text": "5.2 Subgradient in Lasso\nUsing EquationÂ 5.2, the subgradient of f(x) is:\n\\[\n\\partial f(\\beta) = -2X^T \\left( y - X\\beta \\right) + \\lambda \\, \\partial \\left\\| \\beta \\right\\|_1\n\\tag{5.3}\\]\nAnd \\(\\partial \\left\\| \\beta \\right\\|_1\\) is:\n\\[\n\\partial \\left\\| \\beta \\right\\|_1 =\n\\begin{cases}\n1, & \\beta &gt; 0 \\\\\n[-1,1], & \\beta=0 \\\\\n-1, & \\beta &lt; 0\n\\end{cases}\n\\tag{5.4}\\]\nTherefore, The subgradient of the function can be characterized as:\n\\[\n\\begin{cases}\n2X_j^\\top (y - X\\beta) = \\lambda \\operatorname{sign}(\\beta_j) & \\text{if } \\beta_j \\neq 0 \\\\\n|2(X_j)^\\top (y - X\\beta)| \\leq \\lambda & \\text{if } \\beta_j = 0\n\\end{cases}\n\\]\nHere, \\(sign(\\beta)\\) is:\n\\[\nsign(\\beta) =\n\\begin{cases}\n1, & \\beta &gt; 0 \\\\\n-1, & \\beta &lt; 0\n\\end{cases}\n\\]",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#coordinate-descent-method-in-lasso",
    "href": "Project.html#coordinate-descent-method-in-lasso",
    "title": "2Â  Report",
    "section": "6.1 Coordinate Descent Method in Lasso",
    "text": "6.1 Coordinate Descent Method in Lasso\nWhen using the coordinate descent method for EquationÂ 4.2, a key issue arises because the objective function is non-differentiable at \\(x=0\\) due to the absolute value term that is included in the LASSO penalty. The coordinate descent approach addresses this by utilizing soft thredholding.\nAccording to the subgradient condition denoted as EquationÂ 5.3,the formula can be rewritten in the form of EquationÂ 6.1 .This reformulation is crucial in the coordinate descent method, as it optimizes one dimension at a time while other dimensions remain fixed. Making the dimensions explicit is key for the later derivations.\n\\[\n\\frac{\\partial f(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n-2x_{ik} \\left( y_i - \\sum_{j=1}^mx_{ij}\\beta_j \\right) + \\lambda \\, \\frac{\\partial \\sum_{j=1}^m | \\beta_k |}{\\partial \\beta_k}\n\\tag{6.1}\\]\nAnd \\(\\partial \\left\\| \\beta \\right\\|_1\\) is:\n\\[\n\\frac{\\partial \\sum_{j=1}^m | \\beta_k |}{\\partial \\beta_k} =\n\\begin{cases}\n1, & \\beta_k &gt; 0 \\\\\n[-1,1], & \\beta_k=0 \\\\\n-1, & \\beta_k &lt; 0\n\\end{cases}\n\\]\nProceeding with the derivation under the assumption that optimization is conducted along the k-th dimension, taking the partial derivative with respect to this particular dimension yields the following expression:\n\\[\n\\begin{aligned}\n\\frac{\\partial f(\\beta)}{\\partial \\beta_k} &= \\sum_{i=1}^n -2x_{ik} \\left( y_i - \\sum_{j=1}^m x_{ij}\\beta_j \\right) + \\frac{\\partial \\sum_{j=1}^m | \\beta_k |}{\\partial \\beta_k} \\\\\n&= \\sum_{i=1}^n -2x_{ik} \\left( y_i - \\sum_{\\substack{j=1 \\\\ j \\neq k}}^m x_{ij}\\beta_j - x_{ik}\\beta_k \\right) + \\frac{\\partial \\sum_{j=1}^m | \\beta_k |}{\\partial \\beta_k} \\\\\n&= -2 \\sum_{i=1}^n x_{ik} \\left( y_i - \\sum_{\\substack{j=1 \\\\ j \\neq k}}^m x_{ij}\\beta_j \\right) + 2\\beta_k \\sum_{i=1}^n x_{ik}^2 + \\frac{\\partial \\sum_{j=1}^m | \\beta_k |}{\\partial \\beta_k} \\\\\n&= p_k + m_k\\beta_k+\\frac{\\partial \\sum_{j=1}^m | \\beta_k |}{\\partial \\beta_k}\n\\end{aligned}\n\\]\nAssuming that \\(\\beta_k\\) does not equal zero, we equate the partial derivative to zero to find the optimal value of \\(\\beta_k\\), then\n\\[\n\\begin{aligned}\np_k + m_k\\beta_k^*+\\lambda sign(\\beta_k^*)=0 \\\\\n\\Rightarrow \\beta_k^* + \\frac{1}{m_k} (p_k + \\lambda \\text{sign}(\\beta_k^*)) = 0\n\\end{aligned}\n\\]\nFor \\(\\beta_k^*\\), there are three cases to consider based on the value of p_k relative to \\(\\lambda\\) and \\(m_k \\geq 0\\), and it can be written to a solution of \\(\\beta_k^*\\) which is also called soft thredholding (\\(S_{\\lambda}(p_k)\\))\n\\[\nS_{\\lambda}(p_k)=\\beta_k^* =\n\\begin{cases}\n-\\frac{1}{m_k}(p_k - \\lambda) & \\text{if } p_k &gt; \\lambda, \\\\\n-\\frac{1}{m_k}(p_k + \\lambda) & \\text{if } p_k &lt; -\\lambda, \\\\\n0 & \\text{if } -\\lambda \\leq p_k \\leq \\lambda.\n\\end{cases}\n\\tag{6.2}\\]\nEquationÂ 6.2 is a closed form of solution. Therefore, by applying soft thresholding, we can compute the value of \\(p_k\\) and compare it directly with \\(\\lambda\\) to obtain the optimal value of \\(\\beta_k\\). For instance, if the value of \\(p_k\\) falls within the range of \\(-\\lambda\\) to \\(\\lambda\\), then \\(\\beta_k^*\\) will be shrunk to zero.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#coordinate-gradient-algorithm",
    "href": "Project.html#coordinate-gradient-algorithm",
    "title": "2Â  Report",
    "section": "6.2 Coordinate Gradient Algorithm",
    "text": "6.2 Coordinate Gradient Algorithm\nHereâs how it works in the context of coordinate descent for LASSO:\n\nSet k=0 and choose \\(x^0 \\in \\mathbb{R}^n\\);\n\nRepeat\n\nCompute \\(p_k=-2 \\sum_{i=1}^n x_{ik} \\left( y_i - \\sum_{\\substack{j=1 \\\\ j \\neq k}}^m x_{ij}\\beta_j \\right)\\)\nCompute \\(m_k=2\\sum_{i=1}^n x_{ik}^2\\)\nSet \\(\\beta_k=S_{\\lambda}(p_k)\\)\nk=k+1\n\nuntil convergence or max number of iterations;",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#materials-of-proximal-gradient",
    "href": "Project.html#materials-of-proximal-gradient",
    "title": "2Â  Report",
    "section": "7.1 Materials of Proximal Gradient",
    "text": "7.1 Materials of Proximal Gradient\nThe proximal gradient method aims to minimize a composite function of the form:\n\\[\nx = \\arg \\min_{x} g(x) + h(x)\n\\]\nHere, g(x) is a differentiable convex function and h(x) is typically a non-differentiable convex function.\nThe proximal gradient method addresses this optimization problem by considering different characteristics of h(x):\n\nIf h(x)=0, the problem reduces to a standard gradient descent scenario.\nIf \\(h(x)=I_c(x)\\), where \\(I_c(x)\\) is the indicator function for the set C, defined as\n\n\\[\nI_c=\\begin{cases}\n0, & x \\in C\\\\\n\\inftyï¼ & x \\notin C\n\\end{cases}\n\\]\nthen the project gradient descent can be employed to enforce the constraints defined by C.\n\nIf \\(h(x)=\\lambda \\|x\\|_1\\) then then the Iterative Shrinkage-Thresholding Algorithm (ISTA) can be utilized.\n\nIn the context of the Lasso problem, as referenced by EquationÂ 4.2, we often encounter large matrix computations involving terms like \\((X\\top X)^{-1}\\), which are computationally intensive. In the ISTA framework, we address this by considering a modified Lasso problem where the matrix X is omitted, focusing instead on a direct relationship between the predictors and the response variable.\nThe simplified Lasso problem, neglecting the matrix X, is then formulated as:\n\\[\n\\begin{equation}\n\\min_{\\beta} \\left\\{\\left\\| y - \\beta \\right\\|_2^2 + \\lambda \\left\\| \\beta \\right\\|_1 \\right\\}\n\\end{equation}\n\\tag{7.1}\\]\nIn this reformulation, the solution can leverage the soft thresholding operator \\(S_{\\lambda}(b)\\) to efficiently solve the optimization problem. Soft thresholding serves as the proximal operator in this setting, and is defined as:\n\\[\nx^*==\n\\begin{cases}\nb + \\lambda, & b \\leq -\\lambda \\\\\n0, & |b| &lt; \\lambda \\\\\nb - \\lambda, & b \\geq \\lambda\n\\end{cases}\n\\tag{7.2}\\]\nThrough this approach, one can solve the Lasso problem without explicitly involving the matrix X, significantly simplifying the computational process, especially in high-dimensional settings where matrix operations become impractical. This adaptation aligns with the objectives of proximal gradient methods which aim to efficiently handle large-scale optimization problems.",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#lasso-problemçç­ä»·è½¬æ¢",
    "href": "Project.html#lasso-problemçç­ä»·è½¬æ¢",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "7.3 Lasso Problemçç­ä»·è½¬æ¢",
    "text": "7.3 Lasso Problemçç­ä»·è½¬æ¢",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#proximal-gradient-algorithm",
    "href": "Project.html#proximal-gradient-algorithm",
    "title": "2Â  Report",
    "section": "7.3 Proximal Gradient Algorithm",
    "text": "7.3 Proximal Gradient Algorithm\n\nSet k=0 and intial point \\(x^0\\)\n\nRepeat\n\nCompute z by \\(z^{(k)} = x^{(k)} - t\\nabla g(x^{(k)})\\)\nCompute proximal operator by \\(\\text{prox}_{t,h()}(z^{(k)}) = S_{\\lambda}(z^{(k)})\\)\nSet \\(x^{(k+1)}\\) by \\(x^{(k+1)}=\\text{prox}_{t,h()}(z^{(k)})\\)\n\nuntil convergence or max number of iterations;",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "Project.html#numberical-example",
    "href": "Project.html#numberical-example",
    "title": "2Â  LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithms",
    "section": "5.4 Numberical Example",
    "text": "5.4 Numberical Example",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>LASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and  Proximal Gradient Method Algorithms</span>"
    ]
  },
  {
    "objectID": "Project.html#detailed-derivation-of-proximal-gradient-descent-for-lasso",
    "href": "Project.html#detailed-derivation-of-proximal-gradient-descent-for-lasso",
    "title": "2Â  Report",
    "section": "7.2 Detailed Derivation of Proximal Gradient Descent for Lasso",
    "text": "7.2 Detailed Derivation of Proximal Gradient Descent for Lasso\nTo utilize Proximal Gradient Descent as per the soft thresholding equation EquationÂ 7.2, the Lasso problem must be reformulated into a form that does not involve the matrix X.\nFirst, for convex function \\(g(x)\\), according to Lipschitz continuity, for \\(\\forall\\) x, y, there always exists a constant L s.t.\n\\[\n\\left| f'(y) - f'(x) \\right| \\leq t \\|y - x\\|_2.\n\\]\nThen using a second-order Taylor seriesï¼g(x) can be expressed as:\n\\[\n\\begin{align}\ng(x) &= g(x_0) + \\nabla g(x_0)^T (x - x_0) + \\frac{1}{2} \\nabla^2 g(x_0) (x - x_0)^2 \\\\\n&= g(x_0) + \\nabla g(x_0)^T (x - x_0) + \\frac{1}{2t} (x - x_0)^2\n\\end{align}\n\\]\nHence, lasso problem now is\n\\[\n\\arg \\min_{x} \\left[ g(x_0) + \\nabla g(x_0)^T (x - x_0) + \\frac{1}{2t} \\|x - x_0\\|^2_2 + \\lambda \\|x\\|_1 \\right]\n\\tag{7.3}\\]\nSince \\(g(x_0)\\) is constant, EquationÂ 7.3 is equivalent to\n\\[\n\\arg \\min_{x} \\left[ \\nabla g(x_0)^T (x - x_0) + \\frac{1}{2t} \\|x - x_0\\|^2_2 + \\lambda \\|x\\|_1 \\right]\n\\]\nTo balance the terms, we introduce a constant \\((t\\nabla g(x_0))^2\\) .By doing this, the Lasso problem then becomes\n\\[\n\\arg \\min_{x}  \\frac{1}{2t} \\|x - x_0 - t\\nabla g(x_0)\\|^2_2 + \\lambda \\|x\\|_1\n\\]\nDenote \\(z = x_0 - t\\nabla g(x_0)\\)\nThus,\n\\[\nx^{*} = \\arg \\min_{x} \\left( \\|x - z\\|_2^2 + t \\lambda \\|x\\|_1 \\right)\n\\]\nRecall the soft thresholding function \\(S_{\\lambda}(b)\\) denoted as EquationÂ 7.2 , which is used in the context of the Lasso problem, reformulated in the form of EquationÂ 7.1 . The optimal solution \\(x^*\\) can be determined by applying \\(S_{\\lambda}(z)\\), which is known as the proximal operator.\n\\[\n\\text{prox}_{t,h()}(z)=S_{\\lambda}(z)=\n\\begin{cases}\nz + \\lambda, & z \\leq -\\lambda \\\\\n0, & |z| &lt; \\lambda \\\\\nz - \\lambda, & z \\geq \\lambda\n\\end{cases}\n\\]",
    "crumbs": [
      "Term Project",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Convex Optimization",
    "section": "",
    "text": "Preface\nThis is a note book of Convex Optimization and final project\nâLASSO Optimization: A Comparative Study of Subgradient, Coordinate Descent and Proximal Gradient Method Algorithmsâ",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "GradientMethod.html",
    "href": "GradientMethod.html",
    "title": "Gradient Method & Newton Method",
    "section": "",
    "text": "Problem 9.30 (10 points)",
    "crumbs": [
      "Homework",
      "Gradient Method & Newton Method"
    ]
  },
  {
    "objectID": "GradientMethod.html#problem-9.30-10-points",
    "href": "GradientMethod.html#problem-9.30-10-points",
    "title": "Gradient Method & Newton Method",
    "section": "",
    "text": "a\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 20\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\nn_i &lt;- numeric(max_i)\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# for loop\nfor (i in 1:max_i) {\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n  \n  # store iteration values\n  n_i[i] &lt;- i\n  values[i] &lt;- val\n  \n  # stop statement: ||Df(x)||_2 \\leq \\eta\n  if (is.infinite(norm(grad, type = \"2\")) || is.na(norm(grad, type = \"2\"))) {\n    break\n  } else if (norm(grad, type = \"2\") &lt; eta) {\n    values &lt;- values[1:i]  \n    n_i &lt;- n_i[1:i]\n    break\n  }\n  \n  # direction of gradient method\n  v &lt;- -grad\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  \n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  # Ensure to update Ax only once per iteration\n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  # Corrected position of the while loop condition check\n  while (f(new_x, new_Ax) &gt; val + gamma * t * crossprod(grad, v)) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  # Corrected progress message variable\n  cat(\"i:\", i, \"Value:\", val,\"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 0.0625 \ni: 2 Value: -22.20123 Step Length: 1 \ni: 3 Value: -22.93227 Step Length: 8.881784e-16 \ni: 4 Value: -22.93227 Step Length: 4.440892e-16 \ni: 5 Value: -22.93227 Step Length: 1.776357e-15 \n\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Gradient Descent')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Gradient Descent')\n\n\n\n\n\n\n\n\n\n# different size\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 20\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# for loop\nfor (i in 1:max_i) {\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n  \n  # store iteration values\n  values[i] &lt;- val\n  \n  # stop statement: ||Df(x)||_2 \\leq \\eta\n  if (is.infinite(norm(grad, type = \"2\")) || is.na(norm(grad, type = \"2\"))) {\n    break\n  } else if (norm(grad, type = \"2\") &lt; eta) {\n    values &lt;- values[1:i]  \n    n_i &lt;- n_i[1:i]\n    break\n  }\n  \n  # direction of gradient method\n  v &lt;- -grad\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  \n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  # Ensure to update Ax only once per iteration\n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  # Corrected position of the while loop condition check\n  while (f(new_x, new_Ax) &gt; val + gamma * t * crossprod(grad, v)) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  # Corrected progress message variable\n  cat(\"i:\", i, \"Value:\", val,\"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 0.0625 \ni: 2 Value: -21.58727 Step Length: 1 \ni: 3 Value: -21.70757 Step Length: 0.0625 \ni: 4 Value: -21.90264 Step Length: 7.105427e-15 \ni: 5 Value: -21.90264 Step Length: 1.776357e-15 \n\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Gradient Descent')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Gradient Descent')\n\n\n\n\n\n\n\n\n\n\nb\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 10\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  diag_d &lt;- diag(as.vector(d))\n  H &lt;- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian\n  H &lt;- hess(x, Ax, A)\n  \n  # v and &lt;t,v&gt;\n  v &lt;- solve(H, -grad)\n  fprime &lt;- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -3.671415 Step Length: 0.0009765625 \ni: 3 Value: -3.672162 Step Length: 7.105427e-15 \ni: 4 Value: -3.672162 Step Length: 1.387779e-17 \ni: 5 Value: -3.672162 Step Length: 3.552714e-15 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'Iteration', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n\n\n\n\n\n\n\n\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\n\n# size\nm &lt;- 200\nn &lt;- 100\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  diag_d &lt;- diag(as.vector(d))\n  H &lt;- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian\n  H &lt;- hess(x, Ax, A)\n  \n  # v and &lt;t,v&gt;\n  v &lt;- solve(H, -grad)\n  fprime &lt;- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -136.2967 Step Length: 1 \ni: 3 Value: -253.5493 Step Length: 0.25 \ni: 4 Value: -268.9881 Step Length: 1.084202e-19 \ni: 5 Value: -268.9881 Step Length: 1.084202e-19 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'Iteration', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')",
    "crumbs": [
      "Homework",
      "Gradient Method & Newton Method"
    ]
  },
  {
    "objectID": "GradientMethod.html#problem-9.31-10-points",
    "href": "GradientMethod.html#problem-9.31-10-points",
    "title": "Gradient Method & Newton Method",
    "section": "Problem 9.31 (10 points)",
    "text": "Problem 9.31 (10 points)\n\na\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\nN=2\n\n# size\nm &lt;- 10\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\nhess &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  diag_d &lt;- diag(as.vector(d))\n  H &lt;- t(A) %*% diag_d %*% A + diag(1 / ((1 + x)^2)) + diag(1 / ((1 - x)^2))\n  return(H)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\nhessian_counter &lt;- N\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian\n  if (hessian_counter &gt;= N) {\n    H &lt;- hess(x, Ax, A)\n    hessian_counter &lt;- 0 \n  }\n  \n  # v and &lt;t,v&gt;\n  v &lt;- solve(H, -grad)\n  fprime &lt;- t(grad) %*% v\n  \n  # stop statement: \\lambda_2\n  if (abs(fprime) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n\n  # Update Hessian counter\n  hessian_counter &lt;- hessian_counter + 1\n\n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\ni: 1 Value: 0 Step Length: 1 \ni: 2 Value: -4.321432 Step Length: 1 \ni: 3 Value: -5.375923 Step Length: 1.110223e-16 \ni: 4 Value: -5.375923 Step Length: 6.938894e-18 \ni: 5 Value: -5.375923 Step Length: 3.469447e-18 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')\n\n\n\n\n\n\n\n\n\n\nb\n\ngamma &lt;- 0.01\nbeta &lt;- 0.5\nmax_i &lt;- 5\neta &lt;- 0.01\nN=2\n\n# size\nm &lt;- 10\nn &lt;- 10\n\n# initialization\nx &lt;- rep(0, n) \nA &lt;- matrix(runif(m * n), nrow = m)\n\nAx &lt;- A %*% x\n\n# defination of function\nf &lt;- function(x, Ax) {\n  -sum(log(1 - Ax)) - sum(log(1 + x)) - sum(log(1 - x))\n}\n\ngradient &lt;- function(x, Ax, A) {\n  t(A) %*% (1 / (1 - Ax)) - 1 / (1 + x) - 1 / (1 - x)\n}\n\n# Modified\nhess_diag &lt;- function(x, Ax, A) {\n  d &lt;- 1 / ((1 - Ax)^2)\n  # Diagonal Hessian approximation\n  diag_H &lt;- 1 / ((1 - x)^2) + 1 / ((1 + x)^2) + (t(A) * A) %*% d\n  H_diag &lt;- diag(diag(diag_H))\n  return(H_diag)\n}\n\n\n# storage\nvalues &lt;- numeric(max_i)\nsteplen &lt;- numeric(max_i)\n\n# for loop\nfor (i in 1:max_i) {\n  Ax &lt;- A %*% x\n  # f(x)\n  val &lt;- f(x, Ax)\n  \n  # Df(x)\n  grad &lt;- gradient(x, Ax, A)\n\n  # Hessian diagonal approximation\n  H_diag &lt;- hess_diag(x, Ax, A)\n  \n  # Inverse of diag_H approximation\n  H_inv &lt;- 1 / diag(H_diag)\n\n  # v and &lt;t,v&gt;\n  v &lt;- H_inv * (-grad)\n  fprime &lt;- sum(grad * v)\n  \n  # stop statement: \\lambda^2 / 2\n  if (abs(fprime / 2) &lt; eta) {\n    break\n  }\n  \n  # backtracking line search: loop for t\n  t &lt;- 1\n  while (max(A %*% (x + t * v)) &gt;= 1 || max(x + t * v) &gt;= 1 || min(x + t * v) &lt;= -1) {\n    t &lt;- beta * t\n  }\n  \n  new_x &lt;- x + t * v\n  new_Ax &lt;- A %*% new_x\n  \n  while (f(new_x, new_Ax) &gt; val + gamma * t * fprime) {\n    t &lt;- beta * t\n    new_x &lt;- x + t * v\n    new_Ax &lt;- A %*% new_x\n  }\n  \n  # iteration\n  x &lt;- new_x\n  Ax &lt;- new_Ax\n  \n  # store iteration step length\n  steplen[i] &lt;- t\n  \n  cat(\"i:\", i, \"Value:\", val, \"Step Length:\", t, \"\\n\")\n}\n\nWarning in H_inv * (-grad): longer object length is not a multiple of shorter\nobject length\n\n\ni: 1 Value: 0 Step Length: 0.125 \ni: 2 Value: -9.43772 Step Length: 2.775558e-17 \ni: 3 Value: -9.43772 Step Length: 2.775558e-17 \ni: 4 Value: -9.43772 Step Length: 2.775558e-17 \ni: 5 Value: -9.43772 Step Length: 2.775558e-17 \n\n# Plotting\nplot(values, type = 'b', col = 'blue', xlab = 'k', ylab = 'f(x^(k))', main = 'Objective Function During Newton\\'s Method')\n\n\n\n\n\n\n\nplot(steplen, type = 'b', col = 'red', xlab = 'k', ylab = 't^(k)', main = 'Step Length During Newton\\'s Method')",
    "crumbs": [
      "Homework",
      "Gradient Method & Newton Method"
    ]
  }
]